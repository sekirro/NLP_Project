{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPOå¾®è°ƒåå¤§æ¨¡å‹çš„è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€.é—®é¢˜å›ç­”çš„å‡†ç¡®æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡†å¤‡éƒ¨åˆ†\n",
    "å¯¼å…¥ç›¸å…³çš„åº“ï¼Œå®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨å¹¶åˆå§‹åŒ–ï¼Œæœ€åå®šä¹‰è·å–å¤§æ¨¡å‹å›ç­”çš„æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: rouge in ./.local/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "åŠ è½½æ¨¡å‹ä¸­...\n",
      "==((====))==  Unsloth 2024.11.11: Fast Qwen2 patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.72s/it]\n",
      "Unsloth 2024.11.11 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åŠ è½½å¹¶åˆå§‹åŒ–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#å®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "print(\"åŠ è½½æ¨¡å‹ä¸­...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"shared-nvme/llm_models/model_fintuned_DPO/\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ç”¨äºæ¨ç†\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "print(\"æ¨¡å‹åŠ è½½å¹¶åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "\n",
    "#ç”Ÿæˆå›ç­”\n",
    "def generate_answer(model, tokenizer, instruction):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=128,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    start_index = generated_text.rfind('Response:')+len('Response:')\n",
    "    generated_text = generated_text[start_index:]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 ROUGE  \n",
    "ä¸»è¦é€šè¿‡è®¡ç®— n-gram çš„é‡å æ¥è¯„ä¼°æ–‡æœ¬çš„è´¨é‡ã€‚  \n",
    "æ›´å…³æ³¨å¬å›ç‡ï¼Œé€‚åˆè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è¦†ç›–ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:35<00:00,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ROUGEåˆ†æ•°:\n",
      "rouge-1-p: 0.1187\n",
      "rouge-1-r: 0.1191\n",
      "rouge-1-f: 0.1162\n",
      "rouge-2-p: 0.0161\n",
      "rouge-2-r: 0.0162\n",
      "rouge-2-f: 0.0156\n",
      "rouge-l-p: 0.1120\n",
      "rouge-l-r: 0.1125\n",
      "rouge-l-f: 0.1097\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated  rouge-1-f  rouge-2-f  \\\n",
      "0  \\nHello, Thanks for choosing Chat Doctor for p...   0.281879   0.044944   \n",
      "1  \\nHello dear, thank you for your contact to Ch...   0.181818   0.012270   \n",
      "2  \\nHello! Welcome on Chat Doctor ! I understand...   0.202532   0.031250   \n",
      "3  \\nHello, Welcome to Chat Doctor .com I am Chat...   0.224000   0.026490   \n",
      "4  \\næœ¯åæ”¾ç–—ï¼›æ”¾å°„æ²»ç–—ï¼›æ”¾ç–—ï¼›æ‰‹æœ¯è”åˆæ”¾ç–—ï¼›æ‰‹æœ¯æ²»ç–—ï¼›æ”¾åŒ–ç–—ï¼›æ”¾å°„æ²»ç–—è”åˆåŒ–ç–—ï¼›åŒ–ç–—ï¼›æœ¯åæ”¾...   0.000000   0.000000   \n",
      "\n",
      "   rouge-l-f  \n",
      "0   0.268456  \n",
      "1   0.166667  \n",
      "2   0.202532  \n",
      "3   0.192000  \n",
      "4   0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°æ¨¡å‹å¹¶ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "def evaluate_model(model, tokenizer, test_data, num_samples=None):\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # åˆ›å»ºç»“æœåˆ—è¡¨\n",
    "    results = []\n",
    "    \n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®—ROUGEåˆ†æ•°\n",
    "            scores = rouge.get_scores(generated, reference)[0]\n",
    "            \n",
    "            # ä¿å­˜è¯¥æ ·æœ¬çš„æ‰€æœ‰ä¿¡æ¯\n",
    "            result = {\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'rouge-1-p': scores['rouge-1']['p'],\n",
    "                'rouge-1-r': scores['rouge-1']['r'],\n",
    "                'rouge-1-f': scores['rouge-1']['f'],\n",
    "                'rouge-2-p': scores['rouge-2']['p'],\n",
    "                'rouge-2-r': scores['rouge-2']['r'],\n",
    "                'rouge-2-f': scores['rouge-2']['f'],\n",
    "                'rouge-l-p': scores['rouge-l']['p'],\n",
    "                'rouge-l-r': scores['rouge-l']['r'],\n",
    "                'rouge-l-f': scores['rouge-l']['f']\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è¯„ä¼°å‡ºé”™ (è¡Œ {idx}): {e}\")\n",
    "            print(f\"ç”Ÿæˆæ–‡æœ¬: {generated}\")\n",
    "            print(f\"å‚è€ƒæ–‡æœ¬: {reference}\")\n",
    "            continue\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡åˆ†æ•°\n",
    "    avg_scores = {\n",
    "        'rouge-1-p': results_df['rouge-1-p'].mean(),\n",
    "        'rouge-1-r': results_df['rouge-1-r'].mean(),\n",
    "        'rouge-1-f': results_df['rouge-1-f'].mean(),\n",
    "        'rouge-2-p': results_df['rouge-2-p'].mean(),\n",
    "        'rouge-2-r': results_df['rouge-2-r'].mean(),\n",
    "        'rouge-2-f': results_df['rouge-2-f'].mean(),\n",
    "        'rouge-l-p': results_df['rouge-l-p'].mean(),\n",
    "        'rouge-l-r': results_df['rouge-l-r'].mean(),\n",
    "        'rouge-l-f': results_df['rouge-l-f'].mean()\n",
    "    }\n",
    "    \n",
    "    return avg_scores, results_df\n",
    "\n",
    "# ä¸»ç¨‹åº\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. åŠ è½½æ•°æ®\n",
    "    test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "    \n",
    "    # 2. è¯„ä¼°æ¨¡å‹\n",
    "    avg_scores, results_df = evaluate_model(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        test_data,\n",
    "        num_samples=100  # å¯é€‰ï¼šè®¾ç½®æ ·æœ¬æ•°é‡\n",
    "    )\n",
    "    \n",
    "    # 3 ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "    # 3.1 ä¿å­˜æ¯è¡Œç»“æœ\n",
    "    results_df.to_csv('evaluation/finetuned_model/DPO_detailed_rouge_scores.csv', index=False)\n",
    "    # 3.2 ä¿å­˜å¹³å‡åˆ†æ•°\n",
    "    avg_scores_df = pd.DataFrame([avg_scores])\n",
    "    avg_scores_df.to_csv('evaluation/finetuned_model/DPO_average_rouge_scores.csv', index=False)\n",
    "    \n",
    "    # 4. æ‰“å°å¹³å‡åˆ†æ•°\n",
    "    print(\"\\nå¹³å‡ROUGEåˆ†æ•°:\")\n",
    "    for metric, score in avg_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "    # 5. æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "    print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "    print(results_df[['instruction', 'generated', 'rouge-1-f', 'rouge-2-f', 'rouge-l-f']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 BLEU  \n",
    "é€šè¿‡è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ n-gram ç²¾ç¡®åŒ¹é…æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ã€‚  \n",
    "ä½¿ç”¨å‡ ä½•å¹³å‡ç»“åˆä¸åŒé•¿åº¦çš„ n-gram åŒ¹é…ï¼Œå¹¶åŒ…å«æƒ©ç½šå› å­ï¼ˆbrevity penaltyï¼‰ã€‚  \n",
    "æ›´å…³æ³¨ç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆè¯„ä¼°ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚å¸¸ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting sacrebleu\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/15/d8/e51d35bc863caa19ddeae48dfb890581a19326973ad1c9fa5dcfc63310f7/sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/3d/4c/4cb6bb4061910ac74c444be76e7d17dba97d9057030cca2f96947c3f7a0f/portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.24.4)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Installing collected packages: portalocker, colorama, sacrebleu\n",
      "\u001b[33m  WARNING: The script sacrebleu is installed in '/home/pod/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed colorama-0.4.6 portalocker-3.0.0 sacrebleu-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:33<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated      bleu  \n",
      "0  \\nHi! I am Chat Doctor answering your query. F...  2.179670  \n",
      "1  \\nHi thanks for asking question. If your wife ...  0.568534  \n",
      "2  \\nHello dear, Thank you for your contact to Ch...  3.411602  \n",
      "3  \\nHi, Thank you for asking question on Chat Do...  1.711633  \n",
      "4  \\næŠ¤ç†å¹²é¢„ï¼›å¸¸è§„æŠ¤ç†æªæ–½ï¼›å¥åº·æ•™è‚²è·¯å¾„ï¼›å¥åº·æ•™è‚²è·¯å¾„è¡¨ï¼›å¥åº·æ•™è‚²ï¼›å¥åº·å®£æ•™è·¯å¾„ï¼›å¿ƒç†æŠ¤ç†ï¼›...  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "#è®¡ç®— BLEU åˆ†æ•°\n",
    "def evaluate_bleu(model, tokenizer, test_data, num_samples=None):\n",
    "   \n",
    "    results = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·æµ‹è¯•é›†\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•é›†ï¼Œç”Ÿæˆç­”æ¡ˆå¹¶è®¡ç®— BLEU\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®— BLEU åˆ†æ•°\n",
    "            bleu_score = sacrebleu.sentence_bleu(generated, [reference]).score\n",
    "            bleu_scores.append(bleu_score)\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'bleu': bleu_score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ BLEU åˆ†æ•°\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "    return avg_bleu, results_df\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è®¡ç®— BLEU åˆ†æ•°\n",
    "avg_bleu, results_df = evaluate_bleu(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # å¯é€‰ï¼šé™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†åˆ†æ•°ç»“æœ\n",
    "results_df.to_csv('evaluation/finetuned_model/DPO_detailed_bleu_scores.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡åˆ†æ•°ç»“æœ\n",
    "avg_bleu_df = pd.DataFrame([{\"average_bleu\": avg_bleu}])\n",
    "avg_bleu_df.to_csv('evaluation/finetuned_model/DPO_average_bleu_score.csv', index=False)\n",
    "\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'bleu']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU æ›´å…³æ³¨ç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆè¯„ä¼°ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚  \n",
    "è€ŒMETEORè€ƒè™‘äº†è¯åºå’ŒåŒä¹‰è¯æ›¿æ¢ï¼Œæ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå› æ­¤æ›´é€‚åˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 METEOR  \n",
    "é€šè¿‡è®¡ç®—è¯çº§åˆ«çš„åŒ¹é…ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŒ¹é…ã€è¯å¹²åŒ¹é…å’ŒåŒä¹‰è¯åŒ¹é…ã€‚  \n",
    "ä½¿ç”¨è¯åºå’Œè¯ä¹‰ä¿¡æ¯æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ï¼Œç»“åˆäº†ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚  \n",
    "æ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œé€‚åˆè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„å†…å®¹ç›¸å…³æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting nltk\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Installing collected packages: nltk\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/pod/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pod/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/pod/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTKèµ„æºä¸‹è½½å®Œæ¯•ï¼\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# ä¸‹è½½ NLTK èµ„æº\n",
    "nltk.download('wordnet')  # ç”¨äºæ”¯æŒ WordNet è¯æ±‡åº“\n",
    "nltk.download('omw-1.4')  # ç”¨äºæ”¯æŒå¤šè¯­è¨€åŠŸèƒ½\n",
    "print(\"NLTKèµ„æºä¸‹è½½å®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:38<00:00,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ METEOR åˆ†æ•°:\n",
      "METEOR: 0.0855\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated    meteor  \n",
      "0  \\nHello dear, I understand your concern. In my...  0.143392  \n",
      "1  \\nHello! Welcome to Chat Doctor ! I understand...  0.211054  \n",
      "2  \\nHello! Welcome on Chat Doctor ! Regarding yo...  0.185037  \n",
      "3  \\nHello! Welcome on Chat Doctor ! Regarding yo...  0.141482  \n",
      "4  \\næœ¯ååŒ–ç–—ï¼›æœ¯å‰æ”¾ç–—ï¼›æœ¯åæ”¾ç–—ï¼›æ”¾å°„æ€§ç²’å­æ¤å…¥ï¼›ç«‹ä½“å®šå‘æ”¾å°„å¤–ç§‘æ²»ç–—ï¼›ä¼½ç›åˆ€ï¼›æœ¯å‰åŒ–ç–—ï¼›æ‰‹...  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "def evaluate_meteor(model, tokenizer, test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®— METEOR åˆ†æ•°ï¼Œç¡®ä¿ hypothesis å’Œ reference æ˜¯åˆ†è¯åçš„åˆ—è¡¨\n",
    "    :param model: å·²åŠ è½½çš„æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ METEOR åˆ†æ•°å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    meteor_scores = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # åˆ†è¯å¤„ç†\n",
    "            reference_tokens = reference.split()  # å°†å‚è€ƒç­”æ¡ˆåˆ†è¯\n",
    "            generated_tokens = generated.split()  # å°†ç”Ÿæˆæ–‡æœ¬åˆ†è¯\n",
    "\n",
    "            # è®¡ç®— METEOR åˆ†æ•°\n",
    "            score = meteor_score([reference_tokens], generated_tokens)\n",
    "            meteor_scores.append(score)\n",
    "            \n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'meteor': score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ METEOR åˆ†æ•°\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "\n",
    "    return avg_meteor, results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ METEOR è¯„ä¼°é€»è¾‘\n",
    "avg_meteor, results_df = evaluate_meteor(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/DPO_detailed_meteor_scores.csv', index=False)\n",
    "# å°†å¹³å‡åˆ†æ•°ä¿å­˜åˆ°å•ç‹¬çš„ CSV æ–‡ä»¶\n",
    "avg_meteor_df = pd.DataFrame([{\"average_meteor\": avg_meteor}])\n",
    "avg_meteor_df.to_csv('evaluation/finetuned_model/DPO_average_meteor_score.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ METEOR åˆ†æ•°\n",
    "print(\"\\nå¹³å‡ METEOR åˆ†æ•°:\")\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'meteor']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 BERT-Cosin Similarity  \n",
    "BERTScore ä½¿ç”¨ BERT æ¨¡å‹æ¥è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚  \n",
    "é€šè¿‡ BERT çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ï¼Œæ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Similarity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:39<00:00,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\n",
      "Cosine Similarity: 0.8702\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0  \\nHello! Welcome on Chat Doctor ! Regarding yo...           0.886571  \n",
      "1  \\nHi, dairy have gone through your question. I...           0.880157  \n",
      "2  \\nHi thanks for asking question. I have gone t...           0.880757  \n",
      "3  \\nHello, I have studied your case and I think ...           0.898093  \n",
      "4  \\næœ¯åæŠ¤ç†ï¼›å¿ƒç†æŠ¤ç†ï¼›å¥åº·æ•™è‚²ï¼›é¥®é£ŸæŒ‡å¯¼ï¼›ç–¼ç—›æŠ¤ç†ï¼›å¸¸è§„æŠ¤ç†ï¼›æœ¯å‰æŠ¤ç†ï¼›æœ¯åæŠ¤ç†æªæ–½ï¼›å›´æ‰‹...           0.796202  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "local_model_path = \"shared-nvme/llm_models/models--google-bert--bert-base-uncased\"  \n",
    "\n",
    "# åŠ è½½æœ¬åœ° BERT æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bert_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# æ˜¾å¼è®¾ç½®åˆ†è¯å™¨çš„ pad_tokenï¼Œé¿å…é»˜è®¤ä½¿ç”¨ eos_token\n",
    "if bert_tokenizer.pad_token is None:\n",
    "    bert_tokenizer.pad_token = bert_tokenizer.eos_token\n",
    "\n",
    "def compute_sentence_embedding(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç»™å®šæ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„ BERT æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # ç¡®ä¿åŒ…å« attention_mask\n",
    "    inputs['attention_mask'] = inputs.get('attention_mask', None)\n",
    "\n",
    "\n",
    "    # è·å–æ¨¡å‹è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # ä½¿ç”¨ [CLS] token çš„åµŒå…¥ä½œä¸ºå¥å­åµŒå…¥\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bert_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BERT è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Similarity\"):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ä½¿ç”¨å·²æœ‰çš„ generate_answer ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # è®¡ç®—å¥å­åµŒå…¥\n",
    "            reference_embedding = compute_sentence_embedding(reference, bert_model, bert_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding(generated, bert_model, bert_tokenizer)\n",
    "\n",
    "            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ BERT ç›¸ä¼¼åº¦è¯„ä¼°é€»è¾‘\n",
    "avg_cosine_similarity, results_df = evaluate_bert_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/DPO_detailed_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ° CSV æ–‡ä»¶\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/finetuned_model/DPO_average_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "print(\"\\nå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°ä½¿ç”¨åœ¨ä¸­æ–‡ä¸Šè¡¨ç°ä¼˜ç§€çš„çš„BERTè¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æ›´å¥½çš„å¤„ç†åŒä¹‰è¯ã€è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 BGE-Cosin Similarity\n",
    " bge-large-zh-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BGE Similarity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:32<00:00,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\n",
      "Cosine Similarity: 0.6888\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0  \\nHi, Thanks for writing in. I am sorry to hea...           0.788175  \n",
      "1  \\nHi! Welcome to Chat Doctor ! I understand yo...           0.653550  \n",
      "2                                           \\nHello,           0.348475  \n",
      "3  \\nHi, Thanks for your query. I read your query...           0.659393  \n",
      "4  \\nç¥ç»å¯¼èˆªï¼›æœ¯ä¸­ç”µç”Ÿç†ç›‘æµ‹ï¼›æœ¯ä¸­å”¤é†’ï¼›ç¥ç»å¯¼èˆªç³»ç»Ÿï¼›æœ¯ä¸­å”¤é†’éº»é†‰ï¼›æœ¯ä¸­ç¥ç»å¯¼èˆªï¼›æœ¯ä¸­å”¤é†’æŠ€...           0.324012  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "local_model_path = \"shared-nvme/llm_models/models--BAAI--bge-large-zh-v1.5\"  # æ›¿æ¢ä¸ºæ‚¨çš„æœ¬åœ°è·¯å¾„\n",
    "\n",
    "# åŠ è½½æœ¬åœ° BGE æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bge_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bge_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# # æ˜¾å¼è®¾ç½®åˆ†è¯å™¨çš„ pad_token\n",
    "# if bge_tokenizer.pad_token is None:\n",
    "#     bge_tokenizer.pad_token = bge_tokenizer.eos_token  # ä½¿ç”¨ eos_token ä½œä¸º pad_tokenï¼ˆå¦‚æœ‰å¿…è¦ï¼‰\n",
    "\n",
    "def compute_sentence_embedding_bge(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BGE æ¨¡å‹è®¡ç®—æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„ BGE æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # è·å– BGE æ¨¡å‹çš„è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # è·å–æ± åŒ–åµŒå…¥ä½œä¸ºå¥å­ç‰¹å¾\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]  # ä½¿ç”¨ [CLS] ä½œä¸ºå¥å­ç‰¹å¾\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bge_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BGE æ¨¡å‹è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating BGE Similarity\"):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ä½¿ç”¨å…¨å±€çš„ generate_answer å‡½æ•°ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # è®¡ç®—å¥å­åµŒå…¥\n",
    "            reference_embedding = compute_sentence_embedding_bge(reference, bge_model, bge_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding_bge(generated, bge_model, bge_tokenizer)\n",
    "\n",
    "            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ BGE ç›¸ä¼¼åº¦è¯„ä¼°é€»è¾‘\n",
    "avg_cosine_similarity, results_df = evaluate_bge_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/DPO_detailed_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ° CSV æ–‡ä»¶\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/finetuned_model/DPO_average_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "print(\"\\nå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰.é—®é¢˜å›ç­”çš„æµç•…æ€§\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity  \n",
    "è¯„ä¼°æ¨¡å‹çš„è¯­è¨€æµç•…æ€§å’Œè®­ç»ƒè¿‡ç¨‹çš„æ”¶æ•›æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:14<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR èŒƒå›´: ä¸‹é™=0, ä¸Šé™=50.949575662612915\n",
      "è¯†åˆ«åˆ°çš„å¼‚å¸¸å€¼æ•°é‡: 13\n",
      "\n",
      "å¹³å‡ Perplexity:\n",
      "Perplexity: 15.9867\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                                text  perplexity\n",
      "0  Hi, Cannot say in your particular case but loc...   16.237032\n",
      "1  Hi, Thanks for posting your quarry. As you hav...   18.331350\n",
      "2  Hi, Thanks for using Chat Doctor. Your throat ...    7.269117\n",
      "3  Hi. Thanks for your query. Vaginal spotting ma...   17.361469\n",
      "4                                               æŠ¤ç†å¹²é¢„   15.986674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def compute_perplexity(text, model, tokenizer):\n",
    "#     \"\"\"\n",
    "#     ä½¿ç”¨å…¨å±€å®šä¹‰çš„æ¨¡å‹å’Œåˆ†è¯å™¨è®¡ç®—æ–‡æœ¬çš„ Perplexity\n",
    "#     :param text: è¾“å…¥æ–‡æœ¬\n",
    "#     :param model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹\n",
    "#     :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "#     :return: Perplexity å€¼\n",
    "#     \"\"\"\n",
    "#     # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "#     inputs = tokenizer(\n",
    "#         text,\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(\"cuda\")\n",
    "\n",
    "#     # è®¾ç½®è¾“å…¥æ ‡ç­¾\n",
    "#     inputs['labels'] = inputs['input_ids']\n",
    "\n",
    "#     # è®¡ç®—æ¨¡å‹çš„äº¤å‰ç†µæŸå¤±\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = outputs.loss  # æ¨¡å‹è¿”å›çš„äº¤å‰ç†µæŸå¤±\n",
    "\n",
    "#     # æ ¹æ®äº¤å‰ç†µæŸå¤±è®¡ç®— Perplexity\n",
    "#     perplexity = torch.exp(loss).item()\n",
    "#     return perplexity\n",
    "\n",
    "\n",
    "# def evaluate_perplexity(test_data, num_samples=None, z_threshold=3, max_perplexity=100):\n",
    "#     \"\"\"\n",
    "#     è®¡ç®—æµ‹è¯•æ•°æ®é›†ä¸­æ¯æ¡æ–‡æœ¬çš„ Perplexityï¼Œå¹¶è®¡ç®—å¹³å‡å€¼\n",
    "#     æ·»åŠ å¼‚å¸¸å€¼å¤„ç†é€»è¾‘ï¼Œå‰”é™¤ Z-score å¼‚å¸¸å€¼å’Œ Perplexity å¤§äº 100 çš„å€¼\n",
    "#     :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "#     :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "#     :param z_threshold: Z-score é˜ˆå€¼ï¼Œç”¨äºè¿‡æ»¤å¼‚å¸¸å€¼\n",
    "#     :param max_perplexity: Perplexity çš„æœ€å¤§å€¼é˜ˆå€¼\n",
    "#     :return: å¹³å‡ Perplexity å’Œç»“æœ DataFrame\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     perplexities = []\n",
    "\n",
    "#     # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "#     if num_samples and num_samples < len(test_data):\n",
    "#         test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "#     # éå†æµ‹è¯•æ•°æ®é›†\n",
    "#     for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Perplexity\"):\n",
    "#         reference = row['output']  # ä½¿ç”¨å‚è€ƒæ–‡æœ¬è®¡ç®— Perplexity\n",
    "        \n",
    "#         try:\n",
    "#             # è®¡ç®— Perplexity\n",
    "#             perplexity = compute_perplexity(reference, model, tokenizer)\n",
    "#             perplexities.append(perplexity)\n",
    "\n",
    "#             # ä¿å­˜ç»“æœ\n",
    "#             results.append({\n",
    "#                 'text': reference,\n",
    "#                 'perplexity': perplexity\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     # è½¬æ¢ä¸º DataFrame\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     # ä½¿ç”¨ Z-score æ–¹æ³•å‰”é™¤å¼‚å¸¸å€¼\n",
    "#     perplexity_series = pd.Series(perplexities)\n",
    "#     mean_ppl = perplexity_series.mean()\n",
    "#     std_ppl = perplexity_series.std()\n",
    "#     z_scores = (perplexity_series - mean_ppl) / std_ppl\n",
    "#     filtered_perplexities = perplexity_series[(z_scores.abs() <= z_threshold) & (perplexity_series <= max_perplexity)]\n",
    "\n",
    "#     # è®¡ç®—å¹³å‡ Perplexityï¼ˆå‰”é™¤å¼‚å¸¸å€¼ï¼‰\n",
    "#     avg_perplexity = filtered_perplexities.mean() if not filtered_perplexities.empty else 0\n",
    "\n",
    "#     # æ›´æ–° DataFrameï¼ˆå‰”é™¤å¼‚å¸¸å€¼å¯¹åº”çš„è¡Œï¼‰\n",
    "#     results_df['z_score'] = z_scores\n",
    "#     results_df = results_df[(z_scores.abs() <= z_threshold) & (results_df['perplexity'] <= max_perplexity)]\n",
    "\n",
    "#     return avg_perplexity, results_df\n",
    "\n",
    "\n",
    "# # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "# test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# # è°ƒç”¨ Perplexity è¯„ä¼°é€»è¾‘\n",
    "# avg_perplexity, results_df = evaluate_perplexity(\n",
    "#     test_data,\n",
    "#     num_samples=100,  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    "#     z_threshold=3,    # è®¾ç½® Z-score é˜ˆå€¼\n",
    "#     max_perplexity=100  # å‰”é™¤ Perplexity > 100 çš„å€¼\n",
    "# )\n",
    "\n",
    "# # ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "# results_df.to_csv('evaluation/finetuned_model/DPO_detailed_perplexity.csv', index=False)\n",
    "\n",
    "# # ä¿å­˜å¹³å‡ Perplexity åˆ° CSV æ–‡ä»¶\n",
    "# avg_perplexity_df = pd.DataFrame([{\"average_perplexity\": avg_perplexity}])\n",
    "# avg_perplexity_df.to_csv('evaluation/finetuned_model/DPO_average_perplexity.csv', index=False)\n",
    "\n",
    "# # æ‰“å°å¹¶ä¿å­˜å¹³å‡ Perplexity\n",
    "# print(\"\\nå¹³å‡ Perplexity:\")\n",
    "# print(f\"Perplexity: {avg_perplexity:.4f}\")\n",
    "\n",
    "# # æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "# print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "# print(results_df[['text', 'perplexity']].head())\n",
    "\n",
    "\n",
    "def compute_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å…¨å±€å®šä¹‰çš„æ¨¡å‹å’Œåˆ†è¯å™¨è®¡ç®—æ–‡æœ¬çš„ Perplexity\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: Perplexity å€¼\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # è®¾ç½®è¾“å…¥æ ‡ç­¾\n",
    "    inputs['labels'] = inputs['input_ids']\n",
    "\n",
    "    # è®¡ç®—æ¨¡å‹çš„äº¤å‰ç†µæŸå¤±\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss  # æ¨¡å‹è¿”å›çš„äº¤å‰ç†µæŸå¤±\n",
    "\n",
    "    # æ ¹æ®äº¤å‰ç†µæŸå¤±è®¡ç®— Perplexity\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def evaluate_perplexity(test_data, model, tokenizer, num_samples=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æµ‹è¯•æ•°æ®é›†ä¸­æ¯æ¡æ–‡æœ¬çš„ Perplexityï¼Œå¹¶è®¡ç®—å¹³å‡å€¼\n",
    "    ä½¿ç”¨å››åˆ†ä½æ•°ï¼ˆIQRï¼‰åŸåˆ™è¯†åˆ«å¼‚å¸¸å€¼ï¼Œå¯¹å¼‚å¸¸å€¼è¿›è¡Œå¹³æ»‘å¤„ç†\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ Perplexity å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    perplexities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Perplexity\"):\n",
    "        reference = row['output']  # ä½¿ç”¨å‚è€ƒæ–‡æœ¬è®¡ç®— Perplexity\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®— Perplexity\n",
    "            perplexity = compute_perplexity(reference, model, tokenizer)\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "            # ä¿å­˜ç»“æœ\n",
    "            results.append({\n",
    "                'text': reference,\n",
    "                'perplexity': perplexity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®— IQRï¼ˆå››åˆ†ä½é—´è·ï¼‰\n",
    "    perplexity_series = pd.Series(perplexities)\n",
    "    Q1 = perplexity_series.quantile(0.25)  # ç¬¬ 1 å››åˆ†ä½æ•°\n",
    "    Q3 = perplexity_series.quantile(0.75)  # ç¬¬ 3 å››åˆ†ä½æ•°\n",
    "    IQR = max(Q3 - Q1, 1)  # ç¡®ä¿ IQR ä¸ä¸º 0\n",
    "\n",
    "    # æ ¹æ® IQR å®šä¹‰å¼‚å¸¸å€¼èŒƒå›´\n",
    "    lower_bound = max(Q1 - 1.5 * IQR, 0)  # Perplexity ä¸å¯èƒ½å°äº 0\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼ŒéªŒè¯ä¸Šä¸‹é™æ˜¯å¦åˆç†\n",
    "    print(f\"IQR èŒƒå›´: ä¸‹é™={lower_bound}, ä¸Šé™={upper_bound}\")\n",
    "\n",
    "    # è®¡ç®— IQR å†…çš„å‡å€¼\n",
    "    mean_ppl = perplexity_series[(perplexity_series >= lower_bound) & (perplexity_series <= upper_bound)].mean()\n",
    "\n",
    "    # å¯¹å¼‚å¸¸å€¼è¿›è¡Œå¹³æ»‘å¤„ç†ï¼ˆæ›¿æ¢ä¸ºå‡å€¼ï¼‰\n",
    "    smoothed_perplexities = perplexity_series.apply(\n",
    "        lambda x: mean_ppl if x < lower_bound or x > upper_bound else x\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œæ£€æŸ¥å¼‚å¸¸å€¼æ˜¯å¦è¢«è¯†åˆ«\n",
    "    num_anomalies = (perplexity_series < lower_bound).sum() + (perplexity_series > upper_bound).sum()\n",
    "    print(f\"è¯†åˆ«åˆ°çš„å¼‚å¸¸å€¼æ•°é‡: {num_anomalies}\")\n",
    "\n",
    "    # æ›´æ–° DataFrame çš„ Perplexity åˆ—\n",
    "    results_df['perplexity'] = smoothed_perplexities\n",
    "\n",
    "    # è®¡ç®—æœ€ç»ˆçš„å¹³å‡ Perplexity\n",
    "    avg_perplexity = smoothed_perplexities.mean()\n",
    "\n",
    "    return avg_perplexity, results_df\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data_path = 'shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# è°ƒç”¨ Perplexity è¯„ä¼°é€»è¾‘\n",
    "avg_perplexity, results_df = evaluate_perplexity(\n",
    "    test_data,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/DPO_detailed_perplexity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ Perplexity åˆ° CSV æ–‡ä»¶\n",
    "avg_perplexity_df = pd.DataFrame([{\"average_perplexity\": avg_perplexity}])\n",
    "avg_perplexity_df.to_csv('evaluation/finetuned_model/DPO_average_perplexity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ Perplexity\n",
    "print(\"\\nå¹³å‡ Perplexity:\")\n",
    "print(f\"Perplexity: {avg_perplexity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['text', 'perplexity']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
