{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPOå¾®è°ƒåå¤§æ¨¡å‹çš„è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€.é—®é¢˜å›ç­”çš„å‡†ç¡®æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡†å¤‡éƒ¨åˆ†\n",
    "å¯¼å…¥ç›¸å…³çš„åº“ï¼Œå®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨å¹¶åˆå§‹åŒ–ï¼Œæœ€åå®šä¹‰è·å–å¤§æ¨¡å‹å›ç­”çš„æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: rouge in ./.local/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "åŠ è½½æ¨¡å‹ä¸­...\n",
      "==((====))==  Unsloth 2024.11.11: Fast Qwen2 patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.64s/it]\n",
      "Unsloth 2024.11.11 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åŠ è½½å¹¶åˆå§‹åŒ–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#å®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "print(\"åŠ è½½æ¨¡å‹ä¸­...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"ORPO/finetuned_model/\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ç”¨äºæ¨ç†\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "print(\"æ¨¡å‹åŠ è½½å¹¶åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "\n",
    "#ç”Ÿæˆå›ç­”\n",
    "def generate_answer(model, tokenizer, instruction):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=128,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    start_index = generated_text.rfind('Response:')+len('Response:')\n",
    "    generated_text = generated_text[start_index:]\n",
    "    return generated_text\n",
    "\n",
    "# 1. åŠ è½½æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/reward_test/test_csv/reward_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 ROUGE  \n",
    "ä¸»è¦é€šè¿‡è®¡ç®— n-gram çš„é‡å æ¥è¯„ä¼°æ–‡æœ¬çš„è´¨é‡ã€‚  \n",
    "æ›´å…³æ³¨å¬å›ç‡ï¼Œé€‚åˆè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è¦†ç›–ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:27<00:00,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ROUGEåˆ†æ•°:\n",
      "rouge-1-p: 0.0200\n",
      "rouge-1-r: 0.0200\n",
      "rouge-1-f: 0.0200\n",
      "rouge-2-p: 0.0000\n",
      "rouge-2-r: 0.0000\n",
      "rouge-2-f: 0.0000\n",
      "rouge-l-p: 0.0200\n",
      "rouge-l-r: 0.0200\n",
      "rouge-l-f: 0.0200\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0                                    è‚›é—¨ç—…å˜å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…çš„ç—‡çŠ¶?   \n",
      "1  ç™½å¸¦å¢å¤šå‘é»„æ€ä¹ˆä¸ªæƒ…å†µï¼Ÿï¼Œæœ€è¿‘å‡ å¤©ï¼Œæ„Ÿè§‰è‡ªå·±çš„å†…è£¤ä¸€å¤©ä¸‹æ¥ä¼šæœ‰æ¯”è¾ƒå¤šåé»„çš„ç²˜æ¶²ç—•è¿¹ã€‚åˆšå¼€å§‹ä»¥...   \n",
      "2                                      å¤©é›„çš„è¯ç”¨æ¤ç‰©æ ½åŸ¹æ˜¯ä»€ä¹ˆ?   \n",
      "3      è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—ï¼Œæ„å¤–æ€€å­•æ€ä¹ˆåŠä¼¤å®³æ¯”è¾ƒå°è¯·é—®è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—   \n",
      "4                                         è†ºçª—ç©´çš„å®šä½æ˜¯ä»€ä¹ˆ?   \n",
      "\n",
      "                                           generated  rouge-1-f  rouge-2-f  \\\n",
      "0                          ç»“è‚ ç™Œï¼›ç»“ç›´è‚ ç™Œï¼›ç—”ï¼›è‚›å‘¨è„“è‚¿ï¼›å¤§è‚ ç™Œï¼›è‚›ç˜˜ï¼›è‚›è£‚        0.0        0.0   \n",
      "1  ä½ çš„æƒ…å†µè€ƒè™‘æ˜¯ç»†èŒæ€§é˜´é“ç‚ï¼Œå»ºè®®å»åŒ»é™¢åšç™½å¸¦å¸¸è§„ä»”ç»†æ£€æŸ¥è¯Šæ–­ååœ¨åŒ»ç”ŸæŒ‡å¯¼ä¸‹ç”¨è¯æ²»ç–—ï¼Œå¹³æ—¶ç•™æ„...        0.0        0.0   \n",
      "2  1ã€ç”Ÿé•¿ä¹ æ€§ï¼šå¤©é›„ä¸ºå¤šå¹´ç”Ÿè‰æœ¬ï¼Œé«˜çº¦60å˜ç±³ã€‚æ ¹èŒç²—å£®ï¼Œå‘ˆæ‰æŸ±å½¢æˆ–å—çŠ¶ï¼Œå¤–çš®é»„è¤è‰²ï¼Œå†…éƒ¨ç™½è‰²...        0.0        0.0   \n",
      "3  ä½ å¥½ï¼Œæ ¹æ®ä½ çš„æƒ…å†µåˆ†æï¼Œå¦‚æœå‡ºç°è¤è‰²åˆ†æ³Œç‰©çš„è¯ï¼Œæœ‰å¯èƒ½æ˜¯å­å®«æ¢å¤ä¸å¥½å¼•èµ·çš„ã€‚å»ºè®®åˆ°åŒ»é™¢åšBè¶…...        0.0        0.0   \n",
      "4                               åœ¨èƒ¸éƒ¨ï¼Œå½“ç¬¬å››è‚‹é—´éš™ï¼Œå‰æ­£ä¸­çº¿æ—å¼€2å¯¸ã€‚        0.0        0.0   \n",
      "\n",
      "   rouge-l-f  \n",
      "0        0.0  \n",
      "1        0.0  \n",
      "2        0.0  \n",
      "3        0.0  \n",
      "4        0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°æ¨¡å‹å¹¶ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "def evaluate_model(model, tokenizer, test_data, num_samples=None):\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # åˆ›å»ºç»“æœåˆ—è¡¨\n",
    "    results = []\n",
    "    \n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['question']\n",
    "        reference = row['response_chosen']\n",
    "        \n",
    "        # ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        generated = generated.strip()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®—ROUGEåˆ†æ•°\n",
    "            scores = rouge.get_scores(generated, reference)[0]\n",
    "            \n",
    "            # ä¿å­˜è¯¥æ ·æœ¬çš„æ‰€æœ‰ä¿¡æ¯\n",
    "            result = {\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'rouge-1-p': scores['rouge-1']['p'],\n",
    "                'rouge-1-r': scores['rouge-1']['r'],\n",
    "                'rouge-1-f': scores['rouge-1']['f'],\n",
    "                'rouge-2-p': scores['rouge-2']['p'],\n",
    "                'rouge-2-r': scores['rouge-2']['r'],\n",
    "                'rouge-2-f': scores['rouge-2']['f'],\n",
    "                'rouge-l-p': scores['rouge-l']['p'],\n",
    "                'rouge-l-r': scores['rouge-l']['r'],\n",
    "                'rouge-l-f': scores['rouge-l']['f']\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è¯„ä¼°å‡ºé”™ (è¡Œ {idx}): {e}\")\n",
    "            print(f\"ç”Ÿæˆæ–‡æœ¬: {generated}\")\n",
    "            print(f\"å‚è€ƒæ–‡æœ¬: {reference}\")\n",
    "            continue\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡åˆ†æ•°\n",
    "    avg_scores = {\n",
    "        'rouge-1-p': results_df['rouge-1-p'].mean(),\n",
    "        'rouge-1-r': results_df['rouge-1-r'].mean(),\n",
    "        'rouge-1-f': results_df['rouge-1-f'].mean(),\n",
    "        'rouge-2-p': results_df['rouge-2-p'].mean(),\n",
    "        'rouge-2-r': results_df['rouge-2-r'].mean(),\n",
    "        'rouge-2-f': results_df['rouge-2-f'].mean(),\n",
    "        'rouge-l-p': results_df['rouge-l-p'].mean(),\n",
    "        'rouge-l-r': results_df['rouge-l-r'].mean(),\n",
    "        'rouge-l-f': results_df['rouge-l-f'].mean()\n",
    "    }\n",
    "    \n",
    "    return avg_scores, results_df\n",
    "\n",
    "# ä¸»ç¨‹åº\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    # 2. è¯„ä¼°æ¨¡å‹\n",
    "    avg_scores, results_df = evaluate_model(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        test_data,\n",
    "        num_samples=100  # å¯é€‰ï¼šè®¾ç½®æ ·æœ¬æ•°é‡\n",
    "    )\n",
    "    \n",
    "    # 3 ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "    # 3.1 ä¿å­˜æ¯è¡Œç»“æœ\n",
    "    results_df.to_csv('evaluation/finetuned_model/ORPO_detailed_rouge_scores.csv', index=False)\n",
    "    # 3.2 ä¿å­˜å¹³å‡åˆ†æ•°\n",
    "    avg_scores_df = pd.DataFrame([avg_scores])\n",
    "    avg_scores_df.to_csv('evaluation/finetuned_model/ORPO_average_rouge_scores.csv', index=False)\n",
    "    \n",
    "    # 4. æ‰“å°å¹³å‡åˆ†æ•°\n",
    "    print(\"\\nå¹³å‡ROUGEåˆ†æ•°:\")\n",
    "    for metric, score in avg_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "    # 5. æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "    print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "    print(results_df[['instruction', 'generated', 'rouge-1-f', 'rouge-2-f', 'rouge-l-f']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 BLEU  \n",
    "é€šè¿‡è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ n-gram ç²¾ç¡®åŒ¹é…æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ã€‚  \n",
    "ä½¿ç”¨å‡ ä½•å¹³å‡ç»“åˆä¸åŒé•¿åº¦çš„ n-gram åŒ¹é…ï¼Œå¹¶åŒ…å«æƒ©ç½šå› å­ï¼ˆbrevity penaltyï¼‰ã€‚  \n",
    "æ›´å…³æ³¨ç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆè¯„ä¼°ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚å¸¸ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting sacrebleu\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/15/d8/e51d35bc863caa19ddeae48dfb890581a19326973ad1c9fa5dcfc63310f7/sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/3d/4c/4cb6bb4061910ac74c444be76e7d17dba97d9057030cca2f96947c3f7a0f/portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.24.4)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Installing collected packages: portalocker, colorama, sacrebleu\n",
      "\u001b[33m  WARNING: The script sacrebleu is installed in '/home/pod/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed colorama-0.4.6 portalocker-3.0.0 sacrebleu-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:11<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0                                    è‚›é—¨ç—…å˜å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…çš„ç—‡çŠ¶?   \n",
      "1  ç™½å¸¦å¢å¤šå‘é»„æ€ä¹ˆä¸ªæƒ…å†µï¼Ÿï¼Œæœ€è¿‘å‡ å¤©ï¼Œæ„Ÿè§‰è‡ªå·±çš„å†…è£¤ä¸€å¤©ä¸‹æ¥ä¼šæœ‰æ¯”è¾ƒå¤šåé»„çš„ç²˜æ¶²ç—•è¿¹ã€‚åˆšå¼€å§‹ä»¥...   \n",
      "2                                      å¤©é›„çš„è¯ç”¨æ¤ç‰©æ ½åŸ¹æ˜¯ä»€ä¹ˆ?   \n",
      "3      è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—ï¼Œæ„å¤–æ€€å­•æ€ä¹ˆåŠä¼¤å®³æ¯”è¾ƒå°è¯·é—®è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—   \n",
      "4                                         è†ºçª—ç©´çš„å®šä½æ˜¯ä»€ä¹ˆ?   \n",
      "\n",
      "                                           generated      bleu  \n",
      "0                           è‚›é—¨æ¹¿ç–¹ï¼›ç›´è‚ æ¯è‚‰ï¼›ç›´è‚ ç™Œï¼›ç—”ç—…ï¼›å¤–ç—”ï¼›è‚›å‘¨è„“è‚¿  0.000000  \n",
      "1  æ ¹æ®ä½ çš„å™è¿°å’Œç—‡çŠ¶æ¨æµ‹ï¼Œä½ è¿™ç§æƒ…å†µæœ‰å¯èƒ½æ˜¯é˜´é“ç‚å¼•æ¥çš„ã€‚å»ºè®®ä½ æœ€å¥½å»åŒ»é™¢åšå‡ ä¸‹ç™½å¸¦å¸¸è§„ä»”ç»†æ£€...  0.000000  \n",
      "2  å¤©é›„ä¸ºå¤šå¹´ç”Ÿè‰æœ¬ï¼Œé«˜çº¦1mã€‚æ ¹çŠ¶èŒç²—å£®ï¼Œå…·èŠ‚ï¼ŒèŠ‚ä¸Šç€ç”Ÿé¡»æ ¹ã€‚å¶åŸºç”Ÿï¼Œä¸›ç”Ÿï¼›å¶ç‰‡çº¿å½¢æˆ–æŠ«é’ˆå½¢ï¼Œ...  1.139582  \n",
      "3  ä½ å¥½ï¼Œæ ¹æ®ä½ çš„æƒ…å†µç°åœ¨æ˜¯è¯ç‰©æµäº§åçš„ç¬¬10å¤©ï¼Œè¿™ä¸ªæƒ…å†µå»ºè®®ä½ å¯ä»¥åšBè¶…æ£€æŸ¥ä¸€ä¸‹ï¼Œå¦‚æœå­å®«å†…è†œ...  0.000000  \n",
      "4                               åœ¨èƒ¸éƒ¨ï¼Œå¹³ç¬¬5è‚‹é—´éš™ï¼Œå‰æ­£ä¸­çº¿æ—å¼€2å¯¸ã€‚  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "#è®¡ç®— BLEU åˆ†æ•°\n",
    "def evaluate_bleu(model, tokenizer, test_data, num_samples=None):\n",
    "   \n",
    "    results = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·æµ‹è¯•é›†\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•é›†ï¼Œç”Ÿæˆç­”æ¡ˆå¹¶è®¡ç®— BLEU\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['question']\n",
    "        reference = row['response_chosen']\n",
    "        \n",
    "        # ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        generated = generated.strip()\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®— BLEU åˆ†æ•°\n",
    "            bleu_score = sacrebleu.sentence_bleu(generated, [reference]).score\n",
    "            bleu_scores.append(bleu_score)\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'bleu': bleu_score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ BLEU åˆ†æ•°\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "    return avg_bleu, results_df\n",
    "\n",
    "# # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "# test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è®¡ç®— BLEU åˆ†æ•°\n",
    "avg_bleu, results_df = evaluate_bleu(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # å¯é€‰ï¼šé™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†åˆ†æ•°ç»“æœ\n",
    "results_df.to_csv('evaluation/finetuned_model/ORPO_detailed_bleu_scores.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡åˆ†æ•°ç»“æœ\n",
    "avg_bleu_df = pd.DataFrame([{\"average_bleu\": avg_bleu}])\n",
    "avg_bleu_df.to_csv('evaluation/finetuned_model/ORPO_average_bleu_score.csv', index=False)\n",
    "\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'bleu']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU æ›´å…³æ³¨ç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆè¯„ä¼°ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚  \n",
    "è€ŒMETEORè€ƒè™‘äº†è¯åºå’ŒåŒä¹‰è¯æ›¿æ¢ï¼Œæ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå› æ­¤æ›´é€‚åˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 METEOR  \n",
    "é€šè¿‡è®¡ç®—è¯çº§åˆ«çš„åŒ¹é…ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŒ¹é…ã€è¯å¹²åŒ¹é…å’ŒåŒä¹‰è¯åŒ¹é…ã€‚  \n",
    "ä½¿ç”¨è¯åºå’Œè¯ä¹‰ä¿¡æ¯æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ï¼Œç»“åˆäº†ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚  \n",
    "æ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œé€‚åˆè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„å†…å®¹ç›¸å…³æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting nltk\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Installing collected packages: nltk\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/pod/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pod/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/pod/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTKèµ„æºä¸‹è½½å®Œæ¯•ï¼\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# ä¸‹è½½ NLTK èµ„æº\n",
    "nltk.download('wordnet')  # ç”¨äºæ”¯æŒ WordNet è¯æ±‡åº“\n",
    "nltk.download('omw-1.4')  # ç”¨äºæ”¯æŒå¤šè¯­è¨€åŠŸèƒ½\n",
    "print(\"NLTKèµ„æºä¸‹è½½å®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:06<00:00,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ METEOR åˆ†æ•°:\n",
      "METEOR: 0.0100\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0                                    è‚›é—¨ç—…å˜å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…çš„ç—‡çŠ¶?   \n",
      "1  ç™½å¸¦å¢å¤šå‘é»„æ€ä¹ˆä¸ªæƒ…å†µï¼Ÿï¼Œæœ€è¿‘å‡ å¤©ï¼Œæ„Ÿè§‰è‡ªå·±çš„å†…è£¤ä¸€å¤©ä¸‹æ¥ä¼šæœ‰æ¯”è¾ƒå¤šåé»„çš„ç²˜æ¶²ç—•è¿¹ã€‚åˆšå¼€å§‹ä»¥...   \n",
      "2                                      å¤©é›„çš„è¯ç”¨æ¤ç‰©æ ½åŸ¹æ˜¯ä»€ä¹ˆ?   \n",
      "3      è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—ï¼Œæ„å¤–æ€€å­•æ€ä¹ˆåŠä¼¤å®³æ¯”è¾ƒå°è¯·é—®è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—   \n",
      "4                                         è†ºçª—ç©´çš„å®šä½æ˜¯ä»€ä¹ˆ?   \n",
      "\n",
      "                                           generated  meteor  \n",
      "0                                       ç›´è‚ ç™Œï¼›ç—”ç—…ï¼›è‚›ç˜˜ï¼›è‚›è£‚     0.0  \n",
      "1  æ ¹æ®ä½ çš„å™è¿°ï¼Œè€ƒè™‘æ˜¯é˜´é“ç‚çš„å¯èƒ½æ€§å¤§ï¼Œå»ºè®®åˆ°åŒ»é™¢åšåˆ†æ³Œç‰©ä»”ç»†æ£€æŸ¥æ˜ç¡®ç—…å› åå¯¹ç—‡æ²»ç–—ï¼Œå¹³æ—¶æ³¨æ„...     0.0  \n",
      "2  å¤©é›„æ˜¯è–¯è“£ç§‘è–¯è“£å±çš„æ¤ç‰©ï¼Œæ˜¯ä¸­å›½çš„ç‰¹æœ‰æ¤ç‰©ã€‚åˆ†å¸ƒäºä¸­å›½å¤§é™†çš„é™•è¥¿ã€ç”˜è‚ƒã€æ²³å—ã€å››å·ç­‰åœ°ï¼Œç”Ÿé•¿...     0.0  \n",
      "3  ä½ å¥½ï¼Œæ ¹æ®ä½ æè¿°çš„æƒ…å†µçœ‹æ¥å¦‚æœè¯ç‰©æµäº§ä»¥åæœ‰è¤è‰²çš„åˆ†æ³Œç‰©ï¼Œè¯æ˜å®«è…”å†…è¿˜æ˜¯æœ‰æ®‹å­˜çš„ï¼Œå¯ä»¥åšBè¶…...     0.0  \n",
      "4                                     åœ¨èƒ¸éª¨ä¸­çº¿ä¸Šï¼Œå¹³ç¬¬å››è‚‹é—´éš™ã€‚     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import traceback  # å¯¼å…¥ traceback æ¨¡å—ä»¥è·å–å®Œæ•´çš„é”™è¯¯å †æ ˆä¿¡æ¯\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "def evaluate_meteor(model, tokenizer, test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®— METEOR åˆ†æ•°ï¼Œç¡®ä¿ hypothesis å’Œ reference æ˜¯åˆ†è¯åçš„åˆ—è¡¨\n",
    "    :param model: å·²åŠ è½½çš„æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ METEOR åˆ†æ•°å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    meteor_scores = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['question']\n",
    "        reference = row['response_chosen']\n",
    "        \n",
    "        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        generated = generated.strip()\n",
    "\n",
    "        try:\n",
    "            # åˆ†è¯å¤„ç†\n",
    "            reference_tokens = reference.split()  # å°†å‚è€ƒç­”æ¡ˆåˆ†è¯\n",
    "            generated_tokens = generated.split()  # å°†ç”Ÿæˆæ–‡æœ¬åˆ†è¯\n",
    "\n",
    "            # è®¡ç®— METEOR åˆ†æ•°\n",
    "#             score = meteor_score([reference], generated)\n",
    "            score = meteor_score([reference_tokens], generated_tokens)\n",
    "\n",
    "            meteor_scores.append(score)\n",
    "            \n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'meteor': score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            print(\"é”™è¯¯å †æ ˆ:\")\n",
    "            traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆä¿¡æ¯\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ METEOR åˆ†æ•°\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "\n",
    "    return avg_meteor, results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "# test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ METEOR è¯„ä¼°é€»è¾‘\n",
    "avg_meteor, results_df = evaluate_meteor(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/ORPO_detailed_meteor_scores.csv', index=False)\n",
    "# å°†å¹³å‡åˆ†æ•°ä¿å­˜åˆ°å•ç‹¬çš„ CSV æ–‡ä»¶\n",
    "avg_meteor_df = pd.DataFrame([{\"average_meteor\": avg_meteor}])\n",
    "avg_meteor_df.to_csv('evaluation/finetuned_model/ORPO_average_meteor_score.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ METEOR åˆ†æ•°\n",
    "print(\"\\nå¹³å‡ METEOR åˆ†æ•°:\")\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'meteor']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 BERT-Cosin Similarity  \n",
    "BERTScore ä½¿ç”¨ BERT æ¨¡å‹æ¥è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚  \n",
    "é€šè¿‡ BERT çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ï¼Œæ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Similarity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:46<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\n",
      "Cosine Similarity: 0.9290\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0                                    è‚›é—¨ç—…å˜å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…çš„ç—‡çŠ¶?   \n",
      "1  ç™½å¸¦å¢å¤šå‘é»„æ€ä¹ˆä¸ªæƒ…å†µï¼Ÿï¼Œæœ€è¿‘å‡ å¤©ï¼Œæ„Ÿè§‰è‡ªå·±çš„å†…è£¤ä¸€å¤©ä¸‹æ¥ä¼šæœ‰æ¯”è¾ƒå¤šåé»„çš„ç²˜æ¶²ç—•è¿¹ã€‚åˆšå¼€å§‹ä»¥...   \n",
      "2                                      å¤©é›„çš„è¯ç”¨æ¤ç‰©æ ½åŸ¹æ˜¯ä»€ä¹ˆ?   \n",
      "3      è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—ï¼Œæ„å¤–æ€€å­•æ€ä¹ˆåŠä¼¤å®³æ¯”è¾ƒå°è¯·é—®è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—   \n",
      "4                                         è†ºçª—ç©´çš„å®šä½æ˜¯ä»€ä¹ˆ?   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0                                      è‚›é—¨ç˜™ç—’ç—‡ï¼›ç—”ç—…ï¼›ç›´è‚ è„±å‚           0.963428  \n",
      "1  ç™½å¸¦å¢å¤šæ³›é»„ï¼Œè¿™ç§æƒ…å†µå¯èƒ½ä¼šæ˜¯å› ä¸ºé˜´é“ç‚å¯¼è‡´çš„ï¼Œå»ºè®®æ‚¨åŠæ—¶å»åŒ»é™¢æ£€æŸ¥ä¸€ä¸‹ç™½å¸¦å¸¸è§„ï¼Œæ˜ç¡®ç—…å› å...           0.991529  \n",
      "2  å¤©é›„ä¸ºå¤šå¹´ç”Ÿè‰æœ¬ã€‚æ ¹èŒç²—å£®ï¼Œé•¿åœ†æŸ±å½¢ï¼Œæœ‰åˆ†æï¼ŒèŠ‚é—´è†¨å¤§å‘ˆçººé”¤çŠ¶ï¼Œå¤–çš®æ£•è¤è‰²è‡³ç°æ£•è‰²ï¼Œå†…éƒ¨ç™½è‰²...           0.835396  \n",
      "3  ä½ å¥½ï¼Œä¸€èˆ¬åœ¨è¯æµå1å‘¨å·¦å³å°±ä¼šæœ‰è¤è‰²åˆ†æ³Œç‰©çš„ã€‚å¦‚æœè¶…è¿‡è¿™ä¸ªæ—¶é—´çš„è¯ï¼Œå°±å¯èƒ½æ˜¯æ²¡æœ‰æµå¹²å‡€äº†ã€‚å»º...           0.994219  \n",
      "4                               åœ¨èƒ¸éƒ¨ï¼Œå½“ç¬¬4è‚‹é—´éš™ï¼Œå‰æ­£ä¸­çº¿æ—å¼€2å¯¸ã€‚           0.991391  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "local_model_path = \"shared-nvme/llm_models/models--google-bert--bert-base-uncased\"  \n",
    "\n",
    "# åŠ è½½æœ¬åœ° BERT æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bert_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# æ˜¾å¼è®¾ç½®åˆ†è¯å™¨çš„ pad_tokenï¼Œé¿å…é»˜è®¤ä½¿ç”¨ eos_token\n",
    "if bert_tokenizer.pad_token is None:\n",
    "    bert_tokenizer.pad_token = bert_tokenizer.eos_token\n",
    "\n",
    "def compute_sentence_embedding(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç»™å®šæ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„ BERT æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # ç¡®ä¿åŒ…å« attention_mask\n",
    "    inputs['attention_mask'] = inputs.get('attention_mask', None)\n",
    "\n",
    "\n",
    "    # è·å–æ¨¡å‹è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # ä½¿ç”¨ [CLS] token çš„åµŒå…¥ä½œä¸ºå¥å­åµŒå…¥\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bert_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BERT è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Similarity\"):\n",
    "        instruction = row['question']\n",
    "        reference = row['response_chosen']\n",
    "        \n",
    "        # ä½¿ç”¨å·²æœ‰çš„ generate_answer ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        generated = generated.strip()\n",
    "\n",
    "        try:\n",
    "            # è®¡ç®—å¥å­åµŒå…¥\n",
    "            reference_embedding = compute_sentence_embedding(reference, bert_model, bert_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding(generated, bert_model, bert_tokenizer)\n",
    "\n",
    "            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "# test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ BERT ç›¸ä¼¼åº¦è¯„ä¼°é€»è¾‘\n",
    "avg_cosine_similarity, results_df = evaluate_bert_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/ORPO_detailed_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ° CSV æ–‡ä»¶\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/finetuned_model/ORPO_average_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "print(\"\\nå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°ä½¿ç”¨åœ¨ä¸­æ–‡ä¸Šè¡¨ç°ä¼˜ç§€çš„çš„BERTè¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æ›´å¥½çš„å¤„ç†åŒä¹‰è¯ã€è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 BGE-Cosin Similarity\n",
    " bge-large-zh-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BGE Similarity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:38<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\n",
      "Cosine Similarity: 0.7089\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0                                    è‚›é—¨ç—…å˜å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…çš„ç—‡çŠ¶?   \n",
      "1  ç™½å¸¦å¢å¤šå‘é»„æ€ä¹ˆä¸ªæƒ…å†µï¼Ÿï¼Œæœ€è¿‘å‡ å¤©ï¼Œæ„Ÿè§‰è‡ªå·±çš„å†…è£¤ä¸€å¤©ä¸‹æ¥ä¼šæœ‰æ¯”è¾ƒå¤šåé»„çš„ç²˜æ¶²ç—•è¿¹ã€‚åˆšå¼€å§‹ä»¥...   \n",
      "2                                      å¤©é›„çš„è¯ç”¨æ¤ç‰©æ ½åŸ¹æ˜¯ä»€ä¹ˆ?   \n",
      "3      è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—ï¼Œæ„å¤–æ€€å­•æ€ä¹ˆåŠä¼¤å®³æ¯”è¾ƒå°è¯·é—®è¯æµåæœ‰å·§å…‹åŠ›ç™½å¸¦æ˜¯æ²¡æµå¹²å‡€å—   \n",
      "4                                         è†ºçª—ç©´çš„å®šä½æ˜¯ä»€ä¹ˆ?   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0                                   ç›´è‚ ç‚ï¼›è‚›é—¨è£‚ä¼¤ï¼›è‚›é—¨æ¹¿ç–¹ï¼›è‚›ç˜˜           0.308477  \n",
      "1  ä½ çš„æƒ…å†µè€ƒè™‘æ˜¯é˜´é“ç‚å¼•æ¥çš„ï¼Œå»ºè®®ä½ å¯ä»¥ç”¨å¦‡ç‚æ´æ“¦æ´—å¤–é˜´ï¼Œç„¶åå»åŒ»é™¢åšç™½å¸¦å¸¸è§„ä»”ç»†æ£€æŸ¥ï¼Œæ ¹æ®ä»”...           0.913396  \n",
      "2  å¤©é›„ä¸ºç™¾åˆç§‘æ¤ç‰©ã€‚åˆ†å¸ƒäºå±±ä¸œã€æ±Ÿè‹ã€å®‰å¾½ã€æµ™æ±Ÿã€æ±Ÿè¥¿ã€ç¦å»ºã€å°æ¹¾ã€æ²³å—ã€æ¹–åŒ—ã€æ¹–å—ã€å¹¿ä¸œã€å¹¿...           0.718531  \n",
      "3  ä½ å¥½ï¼Œä¸€èˆ¬åœ¨æµäº§åä¸€å‘¨å†…é˜´é“å‡ºè¡€ä¼šé€æ¸åœæ­¢ï¼Œå¦‚æœè¶…è¿‡äº†ä¸€å‘¨ä»æœ‰å‡ºè¡€ï¼Œè€Œä¸”è¡€é‡è¾ƒå¤šçš„è¯ï¼Œæœ‰å¯èƒ½...           0.765814  \n",
      "4                            åœ¨èƒ¸éª¨ä¸­ä¸‹éƒ¨ï¼Œå¹³ç¬¬4è‚‹é—´éš™ï¼Œå‰æ­£ä¸­çº¿æ—å¼€2å¯¸ã€‚           0.838496  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "local_model_path = \"shared-nvme/llm_models/models--BAAI--bge-large-zh-v1.5\"  # æ›¿æ¢ä¸ºæ‚¨çš„æœ¬åœ°è·¯å¾„\n",
    "\n",
    "# åŠ è½½æœ¬åœ° BGE æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bge_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bge_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# # æ˜¾å¼è®¾ç½®åˆ†è¯å™¨çš„ pad_token\n",
    "# if bge_tokenizer.pad_token is None:\n",
    "#     bge_tokenizer.pad_token = bge_tokenizer.eos_token  # ä½¿ç”¨ eos_token ä½œä¸º pad_tokenï¼ˆå¦‚æœ‰å¿…è¦ï¼‰\n",
    "\n",
    "def compute_sentence_embedding_bge(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BGE æ¨¡å‹è®¡ç®—æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„ BGE æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # è·å– BGE æ¨¡å‹çš„è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # è·å–æ± åŒ–åµŒå…¥ä½œä¸ºå¥å­ç‰¹å¾\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]  # ä½¿ç”¨ [CLS] ä½œä¸ºå¥å­ç‰¹å¾\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bge_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BGE æ¨¡å‹è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating BGE Similarity\"):\n",
    "        instruction = row['question']\n",
    "        reference = row['response_chosen']\n",
    "        \n",
    "        # ä½¿ç”¨å…¨å±€çš„ generate_answer å‡½æ•°ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        generated = generated.strip()\n",
    "\n",
    "        try:\n",
    "            # è®¡ç®—å¥å­åµŒå…¥\n",
    "            reference_embedding = compute_sentence_embedding_bge(reference, bge_model, bge_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding_bge(generated, bge_model, bge_tokenizer)\n",
    "\n",
    "            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# # åŠ è½½æµ‹è¯•æ•°æ®\n",
    "# test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ BGE ç›¸ä¼¼åº¦è¯„ä¼°é€»è¾‘\n",
    "avg_cosine_similarity, results_df = evaluate_bge_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/ORPO_detailed_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ° CSV æ–‡ä»¶\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/finetuned_model/ORPO_average_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "print(\"\\nå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰.é—®é¢˜å›ç­”çš„æµç•…æ€§\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity  \n",
    "è¯„ä¼°æ¨¡å‹çš„è¯­è¨€æµç•…æ€§å’Œè®­ç»ƒè¿‡ç¨‹çš„æ”¶æ•›æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:10<00:00,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR èŒƒå›´: ä¸‹é™=0, ä¸Šé™=57.86283826828003\n",
      "è¯†åˆ«åˆ°çš„å¼‚å¸¸å€¼æ•°é‡: 27\n",
      "\n",
      "å¹³å‡ Perplexity:\n",
      "Perplexity: 15.1104\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                                text  perplexity\n",
      "0                                             é£Ÿç®¡å…‹ç½—æ©ç—…   24.213917\n",
      "1  ç™½å¸¦å¢å¤šæ³›é»„åº”è¯¥æ˜¯æ‚£é˜´é“ç‚äº†ï¼Œè€Œä¸”æ˜¯ç»†èŒæ€§é˜´é“ç‚çš„å¯èƒ½æ€§å¤§ï¼Œä½ å¯ä»¥ç”¨å¦‡ç§‘æ´—å‰‚æ“¦æ´—å¤–é˜´é˜´é“ï¼ŒåŒ...    6.597219\n",
      "2  1.æ°”å€™åœŸå£¤ï¼šå¤©é›„å–œæ¸©æš–ã€æ¹¿æ¶¦å’Œå‘é˜³ç¯å¢ƒï¼Œå®œé€‰åœŸå±‚æ·±åšè‚¥æ²ƒã€åœŸè´¨ç–æ¾ã€æ’æ°´è‰¯å¥½çš„åœŸå£¤æ ½ç§ã€‚2...   13.638639\n",
      "3  ä½ çš„è¿™ç§æƒ…å†µå¯èƒ½æ˜¯ç»†èŒæ€§æˆ–éœ‰èŒæ€§é˜´é“ç‚ï¼Œå¯ä»¥ç”¨è‹¦å‚æˆ–åœŸæ§¿çš®ç…æ°´ç†æ´—æ²»ç–—ï¼Œæœ‰ä¸€å®šæ•ˆæœï¼Œè‹¦å‚æˆ–åœŸ...    1.923973\n",
      "4                                     ç¬¬3è‚‹é—´éš™ï¼Œè·å‰æ­£ä¸­çº¿4å¯¸ã€‚   12.033834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å…¨å±€å®šä¹‰çš„æ¨¡å‹å’Œåˆ†è¯å™¨è®¡ç®—æ–‡æœ¬çš„ Perplexity\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: Perplexity å€¼\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # è®¾ç½®è¾“å…¥æ ‡ç­¾\n",
    "    inputs['labels'] = inputs['input_ids']\n",
    "\n",
    "    # è®¡ç®—æ¨¡å‹çš„äº¤å‰ç†µæŸå¤±\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss  # æ¨¡å‹è¿”å›çš„äº¤å‰ç†µæŸå¤±\n",
    "\n",
    "    # æ ¹æ®äº¤å‰ç†µæŸå¤±è®¡ç®— Perplexity\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def evaluate_perplexity(test_data, model, tokenizer, num_samples=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æµ‹è¯•æ•°æ®é›†ä¸­æ¯æ¡æ–‡æœ¬çš„ Perplexityï¼Œå¹¶è®¡ç®—å¹³å‡å€¼\n",
    "    ä½¿ç”¨å››åˆ†ä½æ•°ï¼ˆIQRï¼‰åŸåˆ™è¯†åˆ«å¼‚å¸¸å€¼ï¼Œå¯¹å¼‚å¸¸å€¼è¿›è¡Œå¹³æ»‘å¤„ç†\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ Perplexity å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    perplexities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Perplexity\"):\n",
    "        reference = row['response_chosen']  # ä½¿ç”¨å‚è€ƒæ–‡æœ¬è®¡ç®— Perplexity\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®— Perplexity\n",
    "            perplexity = compute_perplexity(reference, model, tokenizer)\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "            # ä¿å­˜ç»“æœ\n",
    "            results.append({\n",
    "                'text': reference,\n",
    "                'perplexity': perplexity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # æ ‡è®°è¶…å‡ºåˆæ­¥è¿‡æ»¤èŒƒå›´çš„æç«¯å€¼\n",
    "    perplexity_series = pd.Series(perplexities)\n",
    "    MAX_THRESHOLD = 200  # åˆæ­¥è¿‡æ»¤çš„æœ€å¤§å€¼é˜ˆå€¼\n",
    "    is_extreme = perplexity_series > MAX_THRESHOLD  # æ ‡è®°æç«¯å€¼\n",
    "\n",
    "    # æ›¿æ¢è¶…å‡ºèŒƒå›´çš„å€¼ä¸ºä¸´æ—¶æ ‡è®°å€¼ï¼ˆå¦‚ NaNï¼‰\n",
    "    filtered_perplexities = perplexity_series.copy()\n",
    "    filtered_perplexities[is_extreme] = None\n",
    "\n",
    "    \n",
    "    # è®¡ç®—æ–°çš„ IQRï¼ˆå››åˆ†ä½é—´è·ï¼‰\n",
    "    Q1 = filtered_perplexities.quantile(0.25)  # ç¬¬ 1 å››åˆ†ä½æ•°\n",
    "    Q3 = filtered_perplexities.quantile(0.75)  # ç¬¬ 3 å››åˆ†ä½æ•°\n",
    "    IQR = max(Q3 - Q1, 1)  # ç¡®ä¿ IQR ä¸ä¸º 0\n",
    "\n",
    "    # æ ¹æ®æ–°çš„ IQR å®šä¹‰èŒƒå›´\n",
    "    lower_bound = max(Q1 - 1.5 * IQR, 0)  # Perplexity ä¸å¯èƒ½å°äº 0\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼ŒéªŒè¯ä¸Šä¸‹é™æ˜¯å¦åˆç†\n",
    "    print(f\"IQR èŒƒå›´: ä¸‹é™={lower_bound}, ä¸Šé™={upper_bound}\")\n",
    "\n",
    "    # è®¡ç®— IQR å†…çš„å‡å€¼\n",
    "    mean_ppl = perplexity_series[(perplexity_series >= lower_bound) & (perplexity_series <= upper_bound)].mean()\n",
    "\n",
    "    # è®¡ç®— IQR å†…çš„å‡å€¼\n",
    "    mean_ppl = filtered_perplexities.dropna()[\n",
    "        (filtered_perplexities >= lower_bound) & (filtered_perplexities <= upper_bound)\n",
    "    ].mean()\n",
    "\n",
    "    # å¯¹æ‰€æœ‰æç«¯å€¼ï¼ˆæ ‡è®°å€¼å’Œ IQR èŒƒå›´å¤–çš„å€¼ï¼‰è¿›è¡Œå¹³æ»‘å¤„ç†\n",
    "    smoothed_perplexities = perplexity_series.apply(\n",
    "        lambda x: mean_ppl if pd.isna(x) or x < lower_bound or x > upper_bound else x\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œæ£€æŸ¥å¼‚å¸¸å€¼æ˜¯å¦è¢«è¯†åˆ«\n",
    "    num_anomalies = (perplexity_series < lower_bound).sum() + (perplexity_series > upper_bound).sum()\n",
    "    print(f\"è¯†åˆ«åˆ°çš„å¼‚å¸¸å€¼æ•°é‡: {num_anomalies}\")\n",
    "\n",
    "    # æ›´æ–° DataFrame çš„ Perplexity åˆ—\n",
    "    results_df['perplexity'] = smoothed_perplexities\n",
    "\n",
    "    # è®¡ç®—æœ€ç»ˆçš„å¹³å‡ Perplexity\n",
    "    avg_perplexity = smoothed_perplexities.mean()\n",
    "\n",
    "    return avg_perplexity, results_df\n",
    "\n",
    "\n",
    "# è°ƒç”¨ Perplexity è¯„ä¼°é€»è¾‘\n",
    "avg_perplexity, results_df = evaluate_perplexity(\n",
    "    test_data,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/finetuned_model/ORPO_detailed_perplexity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ Perplexity åˆ° CSV æ–‡ä»¶\n",
    "avg_perplexity_df = pd.DataFrame([{\"average_perplexity\": avg_perplexity}])\n",
    "avg_perplexity_df.to_csv('evaluation/finetuned_model/ORPO_average_perplexity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ Perplexity\n",
    "print(\"\\nå¹³å‡ Perplexity:\")\n",
    "print(f\"Perplexity: {avg_perplexity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['text', 'perplexity']].head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
