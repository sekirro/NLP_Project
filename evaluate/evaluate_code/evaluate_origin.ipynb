{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸå§‹æ¨¡å‹çš„è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸€.é—®é¢˜å›ç­”çš„å‡†ç¡®æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡†å¤‡éƒ¨åˆ†\n",
    "å¯¼å…¥ç›¸å…³çš„åº“ï¼Œå®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨å¹¶åˆå§‹åŒ–ï¼Œæœ€åå®šä¹‰è·å–å¤§æ¨¡å‹å›ç­”çš„æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting rouge\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/32/7c/650ae86f92460e9e8ef969cc5008b24798dcf56a9a8947d04c78f550b3f5/rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
      "Installing collected packages: rouge\n",
      "\u001b[33m  WARNING: The script rouge is installed in '/home/pod/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "åŠ è½½æ¨¡å‹ä¸­...\n",
      "==((====))==  Unsloth 2024.11.11: Fast Qwen2 patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åŠ è½½å¹¶åˆå§‹åŒ–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#å®šä¹‰æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæ­¤å¤„æ˜¯åŸå§‹æ¨¡å‹\n",
    "print(\"åŠ è½½æ¨¡å‹ä¸­...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"shared-nvme/llm_models/Qwen2.5-7B-Instruct/\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ç”¨äºæ¨ç†\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "print(\"æ¨¡å‹åŠ è½½å¹¶åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "\n",
    "#ç”Ÿæˆå›ç­”\n",
    "def generate_answer(model, tokenizer, instruction):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=128,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    start_index = generated_text.rfind('Response:')+len('Response:')\n",
    "    generated_text = generated_text[start_index:]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 ROUGE  \n",
    "ä¸»è¦é€šè¿‡è®¡ç®— n-gram çš„é‡å æ¥è¯„ä¼°æ–‡æœ¬çš„è´¨é‡ã€‚  \n",
    "æ›´å…³æ³¨å¬å›ç‡ï¼Œé€‚åˆè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„è¦†ç›–ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:00<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ROUGEåˆ†æ•°:\n",
      "rouge-1-p: 0.0939\n",
      "rouge-1-r: 0.0927\n",
      "rouge-1-f: 0.0915\n",
      "rouge-2-p: 0.0080\n",
      "rouge-2-r: 0.0075\n",
      "rouge-2-f: 0.0074\n",
      "rouge-l-p: 0.0863\n",
      "rouge-l-r: 0.0853\n",
      "rouge-l-f: 0.0841\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated  rouge-1-f  rouge-2-f  \\\n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...   0.193548   0.010309   \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...   0.176471   0.009132   \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...   0.189189   0.011236   \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...   0.233333   0.027586   \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...   0.000000   0.000000   \n",
      "\n",
      "   rouge-l-f  \n",
      "0   0.180645  \n",
      "1   0.152941  \n",
      "2   0.175676  \n",
      "3   0.216667  \n",
      "4   0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°æ¨¡å‹å¹¶ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "def evaluate_model(model, tokenizer, test_data, num_samples=None):\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # åˆ›å»ºç»“æœåˆ—è¡¨\n",
    "    results = []\n",
    "    \n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®—ROUGEåˆ†æ•°\n",
    "            scores = rouge.get_scores(generated, reference)[0]\n",
    "            \n",
    "            # ä¿å­˜è¯¥æ ·æœ¬çš„æ‰€æœ‰ä¿¡æ¯\n",
    "            result = {\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'rouge-1-p': scores['rouge-1']['p'],\n",
    "                'rouge-1-r': scores['rouge-1']['r'],\n",
    "                'rouge-1-f': scores['rouge-1']['f'],\n",
    "                'rouge-2-p': scores['rouge-2']['p'],\n",
    "                'rouge-2-r': scores['rouge-2']['r'],\n",
    "                'rouge-2-f': scores['rouge-2']['f'],\n",
    "                'rouge-l-p': scores['rouge-l']['p'],\n",
    "                'rouge-l-r': scores['rouge-l']['r'],\n",
    "                'rouge-l-f': scores['rouge-l']['f']\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è¯„ä¼°å‡ºé”™ (è¡Œ {idx}): {e}\")\n",
    "            print(f\"ç”Ÿæˆæ–‡æœ¬: {generated}\")\n",
    "            print(f\"å‚è€ƒæ–‡æœ¬: {reference}\")\n",
    "            continue\n",
    "    \n",
    "    # è½¬æ¢ä¸ºDataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡åˆ†æ•°\n",
    "    avg_scores = {\n",
    "        'rouge-1-p': results_df['rouge-1-p'].mean(),\n",
    "        'rouge-1-r': results_df['rouge-1-r'].mean(),\n",
    "        'rouge-1-f': results_df['rouge-1-f'].mean(),\n",
    "        'rouge-2-p': results_df['rouge-2-p'].mean(),\n",
    "        'rouge-2-r': results_df['rouge-2-r'].mean(),\n",
    "        'rouge-2-f': results_df['rouge-2-f'].mean(),\n",
    "        'rouge-l-p': results_df['rouge-l-p'].mean(),\n",
    "        'rouge-l-r': results_df['rouge-l-r'].mean(),\n",
    "        'rouge-l-f': results_df['rouge-l-f'].mean()\n",
    "    }\n",
    "    \n",
    "    return avg_scores, results_df\n",
    "\n",
    "# ä¸»ç¨‹åº\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. åŠ è½½æ•°æ®\n",
    "    test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "    \n",
    "    # 2. è¯„ä¼°æ¨¡å‹\n",
    "    avg_scores, results_df = evaluate_model(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        test_data,\n",
    "        num_samples=100  # å¯é€‰ï¼šè®¾ç½®æ ·æœ¬æ•°é‡\n",
    "    )\n",
    "    \n",
    "    # 3 ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "    # 3.1 ä¿å­˜æ¯è¡Œç»“æœ\n",
    "    results_df.to_csv('evaluation/origin_model/origin_detailed_rouge_scores.csv', index=False)\n",
    "    # 3.2 ä¿å­˜å¹³å‡åˆ†æ•°\n",
    "    avg_scores_df = pd.DataFrame([avg_scores])\n",
    "    avg_scores_df.to_csv('evaluation/origin_model/origin_average_rouge_scores.csv', index=False)\n",
    "    \n",
    "    # 4. æ‰“å°å¹³å‡åˆ†æ•°\n",
    "    print(\"\\nå¹³å‡ROUGEåˆ†æ•°:\")\n",
    "    for metric, score in avg_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "    # 5. æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "    print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "    print(results_df[['instruction', 'generated', 'rouge-1-f', 'rouge-2-f', 'rouge-l-f']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 BLEU  \n",
    "é€šè¿‡è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ n-gram ç²¾ç¡®åŒ¹é…æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ã€‚  \n",
    "ä½¿ç”¨å‡ ä½•å¹³å‡ç»“åˆä¸åŒé•¿åº¦çš„ n-gram åŒ¹é…ï¼Œå¹¶åŒ…å«æƒ©ç½šå› å­ï¼ˆbrevity penaltyï¼‰ã€‚  \n",
    "æ›´å…³æ³¨ç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆè¯„ä¼°ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚å¸¸ç”¨äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.4.3)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.0.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.24.4)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:03<00:00,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated      bleu  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...  1.193548  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...  1.240428  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...  1.502059  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...  1.760298  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "#è®¡ç®— BLEU åˆ†æ•°\n",
    "def evaluate_bleu(model, tokenizer, test_data, num_samples=None):\n",
    "   \n",
    "    results = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·æµ‹è¯•é›†\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•é›†ï¼Œç”Ÿæˆç­”æ¡ˆå¹¶è®¡ç®— BLEU\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®— BLEU åˆ†æ•°\n",
    "            bleu_score = sacrebleu.sentence_bleu(generated, [reference]).score\n",
    "            bleu_scores.append(bleu_score)\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'bleu': bleu_score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ BLEU åˆ†æ•°\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "    return avg_bleu, results_df\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è®¡ç®— BLEU åˆ†æ•°\n",
    "avg_bleu, results_df = evaluate_bleu(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # å¯é€‰ï¼šé™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†åˆ†æ•°ç»“æœ\n",
    "results_df.to_csv('evaluation/finetuned_model/origin_detailed_bleu_scores.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡åˆ†æ•°ç»“æœ\n",
    "avg_bleu_df = pd.DataFrame([{\"average_bleu\": avg_bleu}])\n",
    "avg_bleu_df.to_csv('evaluation/finetuned_model/origin_average_bleu_score.csv', index=False)\n",
    "\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'bleu']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU æ›´å…³æ³¨ç²¾ç¡®åŒ¹é…ï¼Œé€‚åˆè¯„ä¼°ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚  \n",
    "è€ŒMETEORè€ƒè™‘äº†è¯åºå’ŒåŒä¹‰è¯æ›¿æ¢ï¼Œæ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå› æ­¤æ›´é€‚åˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 METEOR  \n",
    "é€šè¿‡è®¡ç®—è¯çº§åˆ«çš„åŒ¹é…ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŒ¹é…ã€è¯å¹²åŒ¹é…å’ŒåŒä¹‰è¯åŒ¹é…ã€‚  \n",
    "ä½¿ç”¨è¯åºå’Œè¯ä¹‰ä¿¡æ¯æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ï¼Œç»“åˆäº†ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚  \n",
    "æ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œé€‚åˆè¯„ä¼°ç”Ÿæˆæ–‡æœ¬çš„å†…å®¹ç›¸å…³æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTKèµ„æºä¸‹è½½å®Œæ¯•ï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pod/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/pod/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# ä¸‹è½½ NLTK èµ„æº\n",
    "nltk.download('wordnet')  # ç”¨äºæ”¯æŒ WordNet è¯æ±‡åº“\n",
    "nltk.download('omw-1.4')  # ç”¨äºæ”¯æŒå¤šè¯­è¨€åŠŸèƒ½\n",
    "print(\"NLTKèµ„æºä¸‹è½½å®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:11<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ METEOR åˆ†æ•°:\n",
      "METEOR: 0.0693\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated    meteor  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...  0.148883  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...  0.102041  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...  0.145509  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...  0.143416  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "def evaluate_meteor(model, tokenizer, test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®— METEOR åˆ†æ•°ï¼Œç¡®ä¿ hypothesis å’Œ reference æ˜¯åˆ†è¯åçš„åˆ—è¡¨\n",
    "    :param model: å·²åŠ è½½çš„æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ METEOR åˆ†æ•°å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    meteor_scores = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # åˆ†è¯å¤„ç†\n",
    "            reference_tokens = reference.split()  # å°†å‚è€ƒç­”æ¡ˆåˆ†è¯\n",
    "            generated_tokens = generated.split()  # å°†ç”Ÿæˆæ–‡æœ¬åˆ†è¯\n",
    "\n",
    "            # è®¡ç®— METEOR åˆ†æ•°\n",
    "            score = meteor_score([reference_tokens], generated_tokens)\n",
    "            meteor_scores.append(score)\n",
    "            \n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'meteor': score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ METEOR åˆ†æ•°\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "\n",
    "    return avg_meteor, results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ METEOR è¯„ä¼°é€»è¾‘\n",
    "avg_meteor, results_df = evaluate_meteor(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_meteor_scores.csv', index=False)\n",
    "# å°†å¹³å‡åˆ†æ•°ä¿å­˜åˆ°å•ç‹¬çš„ CSV æ–‡ä»¶\n",
    "avg_meteor_df = pd.DataFrame([{\"average_meteor\": avg_meteor}])\n",
    "avg_meteor_df.to_csv('evaluation/origin_model/origin_average_meteor_score.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ METEOR åˆ†æ•°\n",
    "print(\"\\nå¹³å‡ METEOR åˆ†æ•°:\")\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'meteor']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 BERTScore  \n",
    "BERTScore ä½¿ç”¨ BERT æ¨¡å‹æ¥è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚  \n",
    "é€šè¿‡ BERT çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡ï¼Œæ›´å…³æ³¨è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Similarity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:46<00:00,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\n",
      "Cosine Similarity: 0.8536\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...           0.931491  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...           0.837991  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...           0.866277  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...           0.932804  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...           0.738793  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "local_model_path = \"shared-nvme/llm_models/models--google-bert--bert-base-uncased\"  \n",
    "\n",
    "# åŠ è½½æœ¬åœ° BERT æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bert_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# æ˜¾å¼è®¾ç½®åˆ†è¯å™¨çš„ pad_tokenï¼Œé¿å…é»˜è®¤ä½¿ç”¨ eos_token\n",
    "if bert_tokenizer.pad_token is None:\n",
    "    bert_tokenizer.pad_token = bert_tokenizer.eos_token\n",
    "\n",
    "def compute_sentence_embedding(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ç»™å®šæ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„ BERT æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # ç¡®ä¿åŒ…å« attention_mask\n",
    "    inputs['attention_mask'] = inputs.get('attention_mask', None)\n",
    "\n",
    "\n",
    "    # è·å–æ¨¡å‹è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # ä½¿ç”¨ [CLS] token çš„åµŒå…¥ä½œä¸ºå¥å­åµŒå…¥\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bert_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BERT è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Similarity\"):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ä½¿ç”¨å·²æœ‰çš„ generate_answer ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # è®¡ç®—å¥å­åµŒå…¥\n",
    "            reference_embedding = compute_sentence_embedding(reference, bert_model, bert_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding(generated, bert_model, bert_tokenizer)\n",
    "\n",
    "            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ BERT ç›¸ä¼¼åº¦è¯„ä¼°é€»è¾‘\n",
    "avg_cosine_similarity, results_df = evaluate_bert_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ° CSV æ–‡ä»¶\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/origin_model/origin_average_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "print(\"\\nå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°ä½¿ç”¨åœ¨ä¸­æ–‡ä¸Šè¡¨ç°ä¼˜ç§€çš„çš„BERTè¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æ›´å¥½çš„å¤„ç†åŒä¹‰è¯ã€è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 BGE-Cosin Similarity\n",
    " bge-large-zh-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BGE Similarity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:41<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\n",
      "Cosine Similarity: 0.6402\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   é¢å¶èƒ¶è´¨ç˜¤æœ¯çš„è¾…åŠ©æ²»ç–—æœ‰äº›ä»€ä¹ˆï¼Ÿ   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...           0.690373  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...           0.655256  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...           0.687346  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...           0.701147  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...           0.249756  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "local_model_path = \"shared-nvme/llm_models/models--BAAI--bge-large-zh-v1.5\"  # æ›¿æ¢ä¸ºæ‚¨çš„æœ¬åœ°è·¯å¾„\n",
    "\n",
    "# åŠ è½½æœ¬åœ° BGE æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "bge_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bge_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# # æ˜¾å¼è®¾ç½®åˆ†è¯å™¨çš„ pad_token\n",
    "# if bge_tokenizer.pad_token is None:\n",
    "#     bge_tokenizer.pad_token = bge_tokenizer.eos_token  # ä½¿ç”¨ eos_token ä½œä¸º pad_tokenï¼ˆå¦‚æœ‰å¿…è¦ï¼‰\n",
    "\n",
    "def compute_sentence_embedding_bge(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BGE æ¨¡å‹è®¡ç®—æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„ BGE æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: æ–‡æœ¬çš„å¥å­åµŒå…¥\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # è·å– BGE æ¨¡å‹çš„è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # è·å–æ± åŒ–åµŒå…¥ä½œä¸ºå¥å­ç‰¹å¾\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]  # ä½¿ç”¨ [CLS] ä½œä¸ºå¥å­ç‰¹å¾\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bge_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ BGE æ¨¡å‹è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating BGE Similarity\"):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # ä½¿ç”¨å…¨å±€çš„ generate_answer å‡½æ•°ç”Ÿæˆå›ç­”\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # è®¡ç®—å¥å­åµŒå…¥\n",
    "            reference_embedding = compute_sentence_embedding_bge(reference, bge_model, bge_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding_bge(generated, bge_model, bge_tokenizer)\n",
    "\n",
    "            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # ä¿å­˜è¯¦ç»†ç»“æœ\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®—å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# è°ƒç”¨ BGE ç›¸ä¼¼åº¦è¯„ä¼°é€»è¾‘\n",
    "avg_cosine_similarity, results_df = evaluate_bge_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ° CSV æ–‡ä»¶\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/origin_model/origin_average_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "print(\"\\nå¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸‰.é—®é¢˜å›ç­”çš„æµç•…æ€§\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity  \n",
    "è¯„ä¼°æ¨¡å‹çš„è¯­è¨€æµç•…æ€§å’Œè®­ç»ƒè¿‡ç¨‹çš„æ”¶æ•›æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR èŒƒå›´: ä¸‹é™=0, ä¸Šé™=52.611210107803345\n",
      "è¯†åˆ«åˆ°çš„å¼‚å¸¸å€¼æ•°é‡: 15\n",
      "\n",
      "å¹³å‡ Perplexity:\n",
      "Perplexity: 16.7350\n",
      "\n",
      "éƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\n",
      "                                                text  perplexity\n",
      "0  Hi, Cannot say in your particular case but loc...   19.448977\n",
      "1  Hi, Thanks for posting your quarry. As you hav...   18.338043\n",
      "2  Hi, Thanks for using Chat Doctor. Your throat ...   11.042951\n",
      "3  Hi. Thanks for your query. Vaginal spotting ma...   19.404900\n",
      "4                                               æŠ¤ç†å¹²é¢„   16.735043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å…¨å±€å®šä¹‰çš„æ¨¡å‹å’Œåˆ†è¯å™¨è®¡ç®—æ–‡æœ¬çš„ Perplexity\n",
    "    :param text: è¾“å…¥æ–‡æœ¬\n",
    "    :param model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :return: Perplexity å€¼\n",
    "    \"\"\"\n",
    "    # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # è®¾ç½®è¾“å…¥æ ‡ç­¾\n",
    "    inputs['labels'] = inputs['input_ids']\n",
    "\n",
    "    # è®¡ç®—æ¨¡å‹çš„äº¤å‰ç†µæŸå¤±\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss  # æ¨¡å‹è¿”å›çš„äº¤å‰ç†µæŸå¤±\n",
    "\n",
    "    # æ ¹æ®äº¤å‰ç†µæŸå¤±è®¡ç®— Perplexity\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def evaluate_perplexity(test_data, model, tokenizer, num_samples=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æµ‹è¯•æ•°æ®é›†ä¸­æ¯æ¡æ–‡æœ¬çš„ Perplexityï¼Œå¹¶è®¡ç®—å¹³å‡å€¼\n",
    "    ä½¿ç”¨å››åˆ†ä½æ•°ï¼ˆIQRï¼‰åŸåˆ™è¯†åˆ«å¼‚å¸¸å€¼ï¼Œå¯¹å¼‚å¸¸å€¼è¿›è¡Œå¹³æ»‘å¤„ç†\n",
    "    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼ˆDataFrameï¼‰\n",
    "    :param model: å·²åŠ è½½çš„è¯­è¨€æ¨¡å‹\n",
    "    :param tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨\n",
    "    :param num_samples: å¯é€‰ï¼Œé™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡\n",
    "    :return: å¹³å‡ Perplexity å’Œç»“æœ DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    perplexities = []\n",
    "\n",
    "    # å¦‚æœéœ€è¦æŠ½æ ·\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # éå†æµ‹è¯•æ•°æ®é›†\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Perplexity\"):\n",
    "        reference = row['output']  # ä½¿ç”¨å‚è€ƒæ–‡æœ¬è®¡ç®— Perplexity\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®— Perplexity\n",
    "            perplexity = compute_perplexity(reference, model, tokenizer)\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "            # ä¿å­˜ç»“æœ\n",
    "            results.append({\n",
    "                'text': reference,\n",
    "                'perplexity': perplexity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"è¡Œ {idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "\n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # è®¡ç®— IQRï¼ˆå››åˆ†ä½é—´è·ï¼‰\n",
    "    perplexity_series = pd.Series(perplexities)\n",
    "    Q1 = perplexity_series.quantile(0.25)  # ç¬¬ 1 å››åˆ†ä½æ•°\n",
    "    Q3 = perplexity_series.quantile(0.75)  # ç¬¬ 3 å››åˆ†ä½æ•°\n",
    "    IQR = max(Q3 - Q1, 1)  # ç¡®ä¿ IQR ä¸ä¸º 0\n",
    "\n",
    "    # æ ¹æ® IQR å®šä¹‰å¼‚å¸¸å€¼èŒƒå›´\n",
    "    lower_bound = max(Q1 - 1.5 * IQR, 0)  # Perplexity ä¸å¯èƒ½å°äº 0\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼ŒéªŒè¯ä¸Šä¸‹é™æ˜¯å¦åˆç†\n",
    "    print(f\"IQR èŒƒå›´: ä¸‹é™={lower_bound}, ä¸Šé™={upper_bound}\")\n",
    "\n",
    "    # è®¡ç®— IQR å†…çš„å‡å€¼\n",
    "    mean_ppl = perplexity_series[(perplexity_series >= lower_bound) & (perplexity_series <= upper_bound)].mean()\n",
    "\n",
    "    # å¯¹å¼‚å¸¸å€¼è¿›è¡Œå¹³æ»‘å¤„ç†ï¼ˆæ›¿æ¢ä¸ºå‡å€¼ï¼‰\n",
    "    smoothed_perplexities = perplexity_series.apply(\n",
    "        lambda x: mean_ppl if x < lower_bound or x > upper_bound else x\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œæ£€æŸ¥å¼‚å¸¸å€¼æ˜¯å¦è¢«è¯†åˆ«\n",
    "    num_anomalies = (perplexity_series < lower_bound).sum() + (perplexity_series > upper_bound).sum()\n",
    "    print(f\"è¯†åˆ«åˆ°çš„å¼‚å¸¸å€¼æ•°é‡: {num_anomalies}\")\n",
    "\n",
    "    # æ›´æ–° DataFrame çš„ Perplexity åˆ—\n",
    "    results_df['perplexity'] = smoothed_perplexities\n",
    "\n",
    "    # è®¡ç®—æœ€ç»ˆçš„å¹³å‡ Perplexity\n",
    "    avg_perplexity = smoothed_perplexities.mean()\n",
    "\n",
    "    return avg_perplexity, results_df\n",
    "\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "test_data_path = 'shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# è°ƒç”¨ Perplexity è¯„ä¼°é€»è¾‘\n",
    "avg_perplexity, results_df = evaluate_perplexity(\n",
    "    test_data,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    num_samples=100  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    ")\n",
    "\n",
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV æ–‡ä»¶\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_perplexity.csv', index=False)\n",
    "\n",
    "# ä¿å­˜å¹³å‡ Perplexity åˆ° CSV æ–‡ä»¶\n",
    "avg_perplexity_df = pd.DataFrame([{\"average_perplexity\": avg_perplexity}])\n",
    "avg_perplexity_df.to_csv('evaluation/origin_model/origin_average_perplexity.csv', index=False)\n",
    "\n",
    "# æ‰“å°å¹¶ä¿å­˜å¹³å‡ Perplexity\n",
    "print(\"\\nå¹³å‡ Perplexity:\")\n",
    "print(f\"Perplexity: {avg_perplexity:.4f}\")\n",
    "\n",
    "# æ‰“å°éƒ¨åˆ†ç¤ºä¾‹ç»“æœ\n",
    "print(\"\\néƒ¨åˆ†ç¤ºä¾‹ç»“æœ:\")\n",
    "print(results_df[['text', 'perplexity']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
