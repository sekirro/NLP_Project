{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始模型的评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.问题回答的准确性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备部分\n",
    "导入相关的库，定义模型和分词器并初始化，最后定义获取大模型回答的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting rouge\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/32/7c/650ae86f92460e9e8ef969cc5008b24798dcf56a9a8947d04c78f550b3f5/rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
      "Installing collected packages: rouge\n",
      "\u001b[33m  WARNING: The script rouge is installed in '/home/pod/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pod/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "加载模型中...\n",
      "==((====))==  Unsloth 2024.11.11: Fast Qwen2 patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载并初始化完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "#定义模型和分词器，此处是原始模型\n",
    "print(\"加载模型中...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"shared-nvme/llm_models/Qwen2.5-7B-Instruct/\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# 初始化模型用于推理\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "print(\"模型加载并初始化完成！\")\n",
    "\n",
    "#生成回答\n",
    "def generate_answer(model, tokenizer, instruction):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=128,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    start_index = generated_text.rfind('Response:')+len('Response:')\n",
    "    generated_text = generated_text[start_index:]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 ROUGE  \n",
    "主要通过计算 n-gram 的重叠来评估文本的质量。  \n",
    "更关注召回率，适合评估生成文本的覆盖率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:00<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均ROUGE分数:\n",
      "rouge-1-p: 0.0939\n",
      "rouge-1-r: 0.0927\n",
      "rouge-1-f: 0.0915\n",
      "rouge-2-p: 0.0080\n",
      "rouge-2-r: 0.0075\n",
      "rouge-2-f: 0.0074\n",
      "rouge-l-p: 0.0863\n",
      "rouge-l-r: 0.0853\n",
      "rouge-l-f: 0.0841\n",
      "\n",
      "部分示例结果:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   额叶胶质瘤术的辅助治疗有些什么？   \n",
      "\n",
      "                                           generated  rouge-1-f  rouge-2-f  \\\n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...   0.193548   0.010309   \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...   0.176471   0.009132   \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...   0.189189   0.011236   \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...   0.233333   0.027586   \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...   0.000000   0.000000   \n",
      "\n",
      "   rouge-l-f  \n",
      "0   0.180645  \n",
      "1   0.152941  \n",
      "2   0.175676  \n",
      "3   0.216667  \n",
      "4   0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估模型并保存详细结果\n",
    "def evaluate_model(model, tokenizer, test_data, num_samples=None):\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # 创建结果列表\n",
    "    results = []\n",
    "    \n",
    "    # 如果需要抽样\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # 对每个样本进行评估\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # 生成回答\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        \n",
    "        try:\n",
    "            # 计算ROUGE分数\n",
    "            scores = rouge.get_scores(generated, reference)[0]\n",
    "            \n",
    "            # 保存该样本的所有信息\n",
    "            result = {\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'rouge-1-p': scores['rouge-1']['p'],\n",
    "                'rouge-1-r': scores['rouge-1']['r'],\n",
    "                'rouge-1-f': scores['rouge-1']['f'],\n",
    "                'rouge-2-p': scores['rouge-2']['p'],\n",
    "                'rouge-2-r': scores['rouge-2']['r'],\n",
    "                'rouge-2-f': scores['rouge-2']['f'],\n",
    "                'rouge-l-p': scores['rouge-l']['p'],\n",
    "                'rouge-l-r': scores['rouge-l']['r'],\n",
    "                'rouge-l-f': scores['rouge-l']['f']\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"评估出错 (行 {idx}): {e}\")\n",
    "            print(f\"生成文本: {generated}\")\n",
    "            print(f\"参考文本: {reference}\")\n",
    "            continue\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 计算平均分数\n",
    "    avg_scores = {\n",
    "        'rouge-1-p': results_df['rouge-1-p'].mean(),\n",
    "        'rouge-1-r': results_df['rouge-1-r'].mean(),\n",
    "        'rouge-1-f': results_df['rouge-1-f'].mean(),\n",
    "        'rouge-2-p': results_df['rouge-2-p'].mean(),\n",
    "        'rouge-2-r': results_df['rouge-2-r'].mean(),\n",
    "        'rouge-2-f': results_df['rouge-2-f'].mean(),\n",
    "        'rouge-l-p': results_df['rouge-l-p'].mean(),\n",
    "        'rouge-l-r': results_df['rouge-l-r'].mean(),\n",
    "        'rouge-l-f': results_df['rouge-l-f'].mean()\n",
    "    }\n",
    "    \n",
    "    return avg_scores, results_df\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 加载数据\n",
    "    test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "    \n",
    "    # 2. 评估模型\n",
    "    avg_scores, results_df = evaluate_model(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        test_data,\n",
    "        num_samples=100  # 可选：设置样本数量\n",
    "    )\n",
    "    \n",
    "    # 3 保存详细结果\n",
    "    # 3.1 保存每行结果\n",
    "    results_df.to_csv('evaluation/origin_model/origin_detailed_rouge_scores.csv', index=False)\n",
    "    # 3.2 保存平均分数\n",
    "    avg_scores_df = pd.DataFrame([avg_scores])\n",
    "    avg_scores_df.to_csv('evaluation/origin_model/origin_average_rouge_scores.csv', index=False)\n",
    "    \n",
    "    # 4. 打印平均分数\n",
    "    print(\"\\n平均ROUGE分数:\")\n",
    "    for metric, score in avg_scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "    # 5. 打印部分示例结果\n",
    "    print(\"\\n部分示例结果:\")\n",
    "    print(results_df[['instruction', 'generated', 'rouge-1-f', 'rouge-2-f', 'rouge-l-f']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 BLEU  \n",
    "通过计算生成文本和参考文本之间的 n-gram 精确匹配来评估文本质量。  \n",
    "使用几何平均结合不同长度的 n-gram 匹配，并包含惩罚因子（brevity penalty）。  \n",
    "更关注精确匹配，适合评估翻译的准确性。常用于机器翻译任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.4.3)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.0.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.24.4)\n",
      "Requirement already satisfied: colorama in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 100/100 [06:03<00:00,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "部分示例结果:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   额叶胶质瘤术的辅助治疗有些什么？   \n",
      "\n",
      "                                           generated      bleu  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...  1.193548  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...  1.240428  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...  1.502059  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...  1.760298  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "#计算 BLEU 分数\n",
    "def evaluate_bleu(model, tokenizer, test_data, num_samples=None):\n",
    "   \n",
    "    results = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    # 如果需要抽样测试集\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # 遍历测试集，生成答案并计算 BLEU\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # 生成回答\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "        \n",
    "        try:\n",
    "            # 计算 BLEU 分数\n",
    "            bleu_score = sacrebleu.sentence_bleu(generated, [reference]).score\n",
    "            bleu_scores.append(bleu_score)\n",
    "            \n",
    "            # 保存结果\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'bleu': bleu_score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"行 {idx} 出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 转换为 DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 计算平均 BLEU 分数\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "    return avg_bleu, results_df\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# 计算 BLEU 分数\n",
    "avg_bleu, results_df = evaluate_bleu(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # 可选：限制样本数量\n",
    ")\n",
    "\n",
    "# 保存详细分数结果\n",
    "results_df.to_csv('evaluation/finetuned_model/origin_detailed_bleu_scores.csv', index=False)\n",
    "\n",
    "# 保存平均分数结果\n",
    "avg_bleu_df = pd.DataFrame([{\"average_bleu\": avg_bleu}])\n",
    "avg_bleu_df.to_csv('evaluation/finetuned_model/origin_average_bleu_score.csv', index=False)\n",
    "\n",
    "\n",
    "# 打印部分示例\n",
    "print(\"\\n部分示例结果:\")\n",
    "print(results_df[['instruction', 'generated', 'bleu']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU 更关注精确匹配，适合评估翻译的准确性。  \n",
    "而METEOR考虑了词序和同义词替换，更关注语义相似性，因此更适合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 METEOR  \n",
    "通过计算词级别的匹配，包括精确匹配、词干匹配和同义词匹配。  \n",
    "使用词序和词义信息来评估文本质量，结合了精确度和召回率。  \n",
    "更关注语义相似性，适合评估生成文本的内容相关性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK资源下载完毕！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pod/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/pod/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# 下载 NLTK 资源\n",
    "nltk.download('wordnet')  # 用于支持 WordNet 词汇库\n",
    "nltk.download('omw-1.4')  # 用于支持多语言功能\n",
    "print(\"NLTK资源下载完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:11<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均 METEOR 分数:\n",
      "METEOR: 0.0693\n",
      "\n",
      "部分示例结果:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   额叶胶质瘤术的辅助治疗有些什么？   \n",
      "\n",
      "                                           generated    meteor  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...  0.148883  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...  0.102041  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...  0.145509  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...  0.143416  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "def evaluate_meteor(model, tokenizer, test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    计算 METEOR 分数，确保 hypothesis 和 reference 是分词后的列表\n",
    "    :param model: 已加载的模型\n",
    "    :param tokenizer: 已加载的分词器\n",
    "    :param test_data: 测试数据集（DataFrame）\n",
    "    :param num_samples: 可选，限制评估样本数量\n",
    "    :return: 平均 METEOR 分数和结果 DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    meteor_scores = []\n",
    "\n",
    "    # 如果需要抽样\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # 遍历测试数据集\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # 使用模型生成回答\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # 分词处理\n",
    "            reference_tokens = reference.split()  # 将参考答案分词\n",
    "            generated_tokens = generated.split()  # 将生成文本分词\n",
    "\n",
    "            # 计算 METEOR 分数\n",
    "            score = meteor_score([reference_tokens], generated_tokens)\n",
    "            meteor_scores.append(score)\n",
    "            \n",
    "            # 保存详细结果\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'meteor': score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"行 {idx} 出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 转换为 DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 计算平均 METEOR 分数\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "\n",
    "    return avg_meteor, results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# 调用 METEOR 评估逻辑\n",
    "avg_meteor, results_df = evaluate_meteor(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    num_samples=100  # 限制样本数量\n",
    ")\n",
    "\n",
    "\n",
    "# 保存详细结果到 CSV 文件\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_meteor_scores.csv', index=False)\n",
    "# 将平均分数保存到单独的 CSV 文件\n",
    "avg_meteor_df = pd.DataFrame([{\"average_meteor\": avg_meteor}])\n",
    "avg_meteor_df.to_csv('evaluation/origin_model/origin_average_meteor_score.csv', index=False)\n",
    "\n",
    "# 打印并保存平均 METEOR 分数\n",
    "print(\"\\n平均 METEOR 分数:\")\n",
    "print(f\"METEOR: {avg_meteor:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 打印部分示例结果\n",
    "print(\"\\n部分示例结果:\")\n",
    "print(results_df[['instruction', 'generated', 'meteor']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 BERTScore  \n",
    "BERTScore 使用 BERT 模型来计算生成文本和参考文本之间的语义相似性。  \n",
    "通过 BERT 的上下文信息来评估文本质量，更关注语义相似性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Similarity: 100%|██████████| 100/100 [05:46<00:00,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均余弦相似度:\n",
      "Cosine Similarity: 0.8536\n",
      "\n",
      "部分示例结果:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   额叶胶质瘤术的辅助治疗有些什么？   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...           0.931491  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...           0.837991  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...           0.866277  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...           0.932804  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...           0.738793  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 指定本地模型路径\n",
    "local_model_path = \"shared-nvme/llm_models/models--google-bert--bert-base-uncased\"  \n",
    "\n",
    "# 加载本地 BERT 模型和分词器\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bert_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# 显式设置分词器的 pad_token，避免默认使用 eos_token\n",
    "if bert_tokenizer.pad_token is None:\n",
    "    bert_tokenizer.pad_token = bert_tokenizer.eos_token\n",
    "\n",
    "def compute_sentence_embedding(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    计算给定文本的句子嵌入\n",
    "    :param text: 输入文本\n",
    "    :param model: 已加载的 BERT 模型\n",
    "    :param tokenizer: 已加载的分词器\n",
    "    :return: 文本的句子嵌入\n",
    "    \"\"\"\n",
    "    # 对文本进行编码\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 确保包含 attention_mask\n",
    "    inputs['attention_mask'] = inputs.get('attention_mask', None)\n",
    "\n",
    "\n",
    "    # 获取模型输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # 使用 [CLS] token 的嵌入作为句子嵌入\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bert_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    使用 BERT 计算生成文本和参考文本之间的余弦相似度\n",
    "    :param test_data: 测试数据集（DataFrame）\n",
    "    :param num_samples: 可选，限制评估样本数量\n",
    "    :return: 平均余弦相似度和结果 DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # 如果需要抽样\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # 遍历测试数据集\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Similarity\"):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # 使用已有的 generate_answer 生成回答\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # 计算句子嵌入\n",
    "            reference_embedding = compute_sentence_embedding(reference, bert_model, bert_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding(generated, bert_model, bert_tokenizer)\n",
    "\n",
    "            # 计算余弦相似度\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # 保存详细结果\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"行 {idx} 出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 转换为 DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 计算平均余弦相似度\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# 调用 BERT 相似度评估逻辑\n",
    "avg_cosine_similarity, results_df = evaluate_bert_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # 限制样本数量\n",
    ")\n",
    "\n",
    "# 保存详细结果到 CSV 文件\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# 保存平均余弦相似度到 CSV 文件\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/origin_model/origin_average_bert_cosine_similarity.csv', index=False)\n",
    "\n",
    "# 打印并保存平均余弦相似度\n",
    "print(\"\\n平均余弦相似度:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# 打印部分示例结果\n",
    "print(\"\\n部分示例结果:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到使用在中文上表现优秀的的BERT语言模型，可以更好的处理同义词、语义上的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 BGE-Cosin Similarity\n",
    " bge-large-zh-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BGE Similarity: 100%|██████████| 100/100 [05:41<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均余弦相似度:\n",
      "Cosine Similarity: 0.6402\n",
      "\n",
      "部分示例结果:\n",
      "                                         instruction  \\\n",
      "0  If you are a doctor, please answer the medical...   \n",
      "1  If you are a doctor, please answer the medical...   \n",
      "2  If you are a doctor, please answer the medical...   \n",
      "3  If you are a doctor, please answer the medical...   \n",
      "4                                   额叶胶质瘤术的辅助治疗有些什么？   \n",
      "\n",
      "                                           generated  cosine_similarity  \n",
      "0  ou are Qwen, created by Alibaba Cloud. You are...           0.690373  \n",
      "1  ou are Qwen, created by Alibaba Cloud. You are...           0.655256  \n",
      "2  ou are Qwen, created by Alibaba Cloud. You are...           0.687346  \n",
      "3  ou are Qwen, created by Alibaba Cloud. You are...           0.701147  \n",
      "4  ou are Qwen, created by Alibaba Cloud. You are...           0.249756  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 指定本地模型路径\n",
    "local_model_path = \"shared-nvme/llm_models/models--BAAI--bge-large-zh-v1.5\"  # 替换为您的本地路径\n",
    "\n",
    "# 加载本地 BGE 模型和分词器\n",
    "bge_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "bge_model = AutoModel.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "# # 显式设置分词器的 pad_token\n",
    "# if bge_tokenizer.pad_token is None:\n",
    "#     bge_tokenizer.pad_token = bge_tokenizer.eos_token  # 使用 eos_token 作为 pad_token（如有必要）\n",
    "\n",
    "def compute_sentence_embedding_bge(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    使用 BGE 模型计算文本的句子嵌入\n",
    "    :param text: 输入文本\n",
    "    :param model: 已加载的 BGE 模型\n",
    "    :param tokenizer: 已加载的分词器\n",
    "    :return: 文本的句子嵌入\n",
    "    \"\"\"\n",
    "    # 对文本进行编码\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 获取 BGE 模型的输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # 获取池化嵌入作为句子特征\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :]  # 使用 [CLS] 作为句子特征\n",
    "    return sentence_embedding\n",
    "\n",
    "def evaluate_bge_similarity(test_data, num_samples=None):\n",
    "    \"\"\"\n",
    "    使用 BGE 模型计算生成文本和参考文本之间的余弦相似度\n",
    "    :param test_data: 测试数据集（DataFrame）\n",
    "    :param num_samples: 可选，限制评估样本数量\n",
    "    :return: 平均余弦相似度和结果 DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # 如果需要抽样\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # 遍历测试数据集\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating BGE Similarity\"):\n",
    "        instruction = row['instruction']\n",
    "        reference = row['output']\n",
    "        \n",
    "        # 使用全局的 generate_answer 函数生成回答\n",
    "        generated = generate_answer(model, tokenizer, instruction)\n",
    "\n",
    "        try:\n",
    "            # 计算句子嵌入\n",
    "            reference_embedding = compute_sentence_embedding_bge(reference, bge_model, bge_tokenizer)\n",
    "            generated_embedding = compute_sentence_embedding_bge(generated, bge_model, bge_tokenizer)\n",
    "\n",
    "            # 计算余弦相似度\n",
    "            cosine_similarity = torch.nn.functional.cosine_similarity(\n",
    "                reference_embedding, generated_embedding\n",
    "            ).item()\n",
    "            cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "            # 保存详细结果\n",
    "            results.append({\n",
    "                'instruction': instruction,\n",
    "                'reference': reference,\n",
    "                'generated': generated,\n",
    "                'cosine_similarity': cosine_similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"行 {idx} 出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 转换为 DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 计算平均余弦相似度\n",
    "    avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities) if cosine_similarities else 0\n",
    "\n",
    "    return avg_cosine_similarity, results_df\n",
    "\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = pd.read_csv('shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv')\n",
    "\n",
    "# 调用 BGE 相似度评估逻辑\n",
    "avg_cosine_similarity, results_df = evaluate_bge_similarity(\n",
    "    test_data,\n",
    "    num_samples=100  # 限制样本数量\n",
    ")\n",
    "\n",
    "# 保存详细结果到 CSV 文件\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# 保存平均余弦相似度到 CSV 文件\n",
    "avg_cosine_similarity_df = pd.DataFrame([{\"average_cosine_similarity\": avg_cosine_similarity}])\n",
    "avg_cosine_similarity_df.to_csv('evaluation/origin_model/origin_average_bge_cosine_similarity.csv', index=False)\n",
    "\n",
    "# 打印并保存平均余弦相似度\n",
    "print(\"\\n平均余弦相似度:\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "\n",
    "# 打印部分示例结果\n",
    "print(\"\\n部分示例结果:\")\n",
    "print(results_df[['instruction', 'generated', 'cosine_similarity']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三.问题回答的流畅性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity  \n",
    "评估模型的语言流畅性和训练过程的收敛性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 100/100 [00:08<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR 范围: 下限=0, 上限=52.611210107803345\n",
      "识别到的异常值数量: 15\n",
      "\n",
      "平均 Perplexity:\n",
      "Perplexity: 16.7350\n",
      "\n",
      "部分示例结果:\n",
      "                                                text  perplexity\n",
      "0  Hi, Cannot say in your particular case but loc...   19.448977\n",
      "1  Hi, Thanks for posting your quarry. As you hav...   18.338043\n",
      "2  Hi, Thanks for using Chat Doctor. Your throat ...   11.042951\n",
      "3  Hi. Thanks for your query. Vaginal spotting ma...   19.404900\n",
      "4                                               护理干预   16.735043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    使用全局定义的模型和分词器计算文本的 Perplexity\n",
    "    :param text: 输入文本\n",
    "    :param model: 已加载的语言模型\n",
    "    :param tokenizer: 已加载的分词器\n",
    "    :return: Perplexity 值\n",
    "    \"\"\"\n",
    "    # 对文本进行编码\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 设置输入标签\n",
    "    inputs['labels'] = inputs['input_ids']\n",
    "\n",
    "    # 计算模型的交叉熵损失\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss  # 模型返回的交叉熵损失\n",
    "\n",
    "    # 根据交叉熵损失计算 Perplexity\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def evaluate_perplexity(test_data, model, tokenizer, num_samples=None):\n",
    "    \"\"\"\n",
    "    计算测试数据集中每条文本的 Perplexity，并计算平均值\n",
    "    使用四分位数（IQR）原则识别异常值，对异常值进行平滑处理\n",
    "    :param test_data: 测试数据集（DataFrame）\n",
    "    :param model: 已加载的语言模型\n",
    "    :param tokenizer: 已加载的分词器\n",
    "    :param num_samples: 可选，限制评估样本数量\n",
    "    :return: 平均 Perplexity 和结果 DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    perplexities = []\n",
    "\n",
    "    # 如果需要抽样\n",
    "    if num_samples and num_samples < len(test_data):\n",
    "        test_data = test_data.sample(n=num_samples, random_state=42)\n",
    "    \n",
    "    # 遍历测试数据集\n",
    "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Calculating Perplexity\"):\n",
    "        reference = row['output']  # 使用参考文本计算 Perplexity\n",
    "        \n",
    "        try:\n",
    "            # 计算 Perplexity\n",
    "            perplexity = compute_perplexity(reference, model, tokenizer)\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "            # 保存结果\n",
    "            results.append({\n",
    "                'text': reference,\n",
    "                'perplexity': perplexity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"行 {idx} 出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    # 转换为 DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 计算 IQR（四分位间距）\n",
    "    perplexity_series = pd.Series(perplexities)\n",
    "    Q1 = perplexity_series.quantile(0.25)  # 第 1 四分位数\n",
    "    Q3 = perplexity_series.quantile(0.75)  # 第 3 四分位数\n",
    "    IQR = max(Q3 - Q1, 1)  # 确保 IQR 不为 0\n",
    "\n",
    "    # 根据 IQR 定义异常值范围\n",
    "    lower_bound = max(Q1 - 1.5 * IQR, 0)  # Perplexity 不可能小于 0\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # 输出调试信息，验证上下限是否合理\n",
    "    print(f\"IQR 范围: 下限={lower_bound}, 上限={upper_bound}\")\n",
    "\n",
    "    # 计算 IQR 内的均值\n",
    "    mean_ppl = perplexity_series[(perplexity_series >= lower_bound) & (perplexity_series <= upper_bound)].mean()\n",
    "\n",
    "    # 对异常值进行平滑处理（替换为均值）\n",
    "    smoothed_perplexities = perplexity_series.apply(\n",
    "        lambda x: mean_ppl if x < lower_bound or x > upper_bound else x\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # 输出调试信息，检查异常值是否被识别\n",
    "    num_anomalies = (perplexity_series < lower_bound).sum() + (perplexity_series > upper_bound).sum()\n",
    "    print(f\"识别到的异常值数量: {num_anomalies}\")\n",
    "\n",
    "    # 更新 DataFrame 的 Perplexity 列\n",
    "    results_df['perplexity'] = smoothed_perplexities\n",
    "\n",
    "    # 计算最终的平均 Perplexity\n",
    "    avg_perplexity = smoothed_perplexities.mean()\n",
    "\n",
    "    return avg_perplexity, results_df\n",
    "\n",
    "\n",
    "# 加载测试数据\n",
    "test_data_path = 'shared-nvme/datasets/achieve/finetune_test/test_csv/finetune_test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# 调用 Perplexity 评估逻辑\n",
    "avg_perplexity, results_df = evaluate_perplexity(\n",
    "    test_data,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    num_samples=100  # 限制样本数量\n",
    ")\n",
    "\n",
    "# 保存详细结果到 CSV 文件\n",
    "results_df.to_csv('evaluation/origin_model/origin_detailed_perplexity.csv', index=False)\n",
    "\n",
    "# 保存平均 Perplexity 到 CSV 文件\n",
    "avg_perplexity_df = pd.DataFrame([{\"average_perplexity\": avg_perplexity}])\n",
    "avg_perplexity_df.to_csv('evaluation/origin_model/origin_average_perplexity.csv', index=False)\n",
    "\n",
    "# 打印并保存平均 Perplexity\n",
    "print(\"\\n平均 Perplexity:\")\n",
    "print(f\"Perplexity: {avg_perplexity:.4f}\")\n",
    "\n",
    "# 打印部分示例结果\n",
    "print(\"\\n部分示例结果:\")\n",
    "print(results_df[['text', 'perplexity']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
