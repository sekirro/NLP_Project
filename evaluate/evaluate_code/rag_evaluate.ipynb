{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model ok\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "# from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 10000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "llm_path = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_path,\n",
    "            device_map=\"auto\",  # 自动分配设备\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,  # 使用float16精度\n",
    "            load_in_4bit=load_in_4bit,  # 4bit量化\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                llm_path,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"model ok\")\n",
    "\n",
    "embeddings_path = 'BAAI/bge-large-zh-v1.5'\n",
    "# embeddings_path = \"/home/ubuntu/embedding_models/bge-large-zh-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_path,\n",
    "    model_kwargs={\n",
    "        'device': device,\n",
    "        'local_files_only': True  # 指定使用本地模型\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True,\n",
    "        'batch_size': 32\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "#         max_length=10000,\n",
    "        max_new_tokens=500,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15,\n",
    "        no_repeat_ngram_size=3,    # 避免重复生成n-gram\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # 重要：禁用截断\n",
    "        truncation=False,\n",
    "        # 处理长文本\n",
    "        return_full_text=True\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oss2\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import docx\n",
    "import os\n",
    "import chardet\n",
    "import javalang\n",
    "import ast\n",
    "import esprima\n",
    "import clang\n",
    "## 文件处理相关函数\n",
    "def init_oss():\n",
    "    # OSS初始化配置\n",
    "    accessKeyId = os.getenv('ACCESSKEY_ID')\n",
    "    accessKeySecret = os.getenv('ACCESSKEY_SECRET')\n",
    "    auth = oss2.Auth(accessKeyId, accessKeySecret)\n",
    "\n",
    "    endpoint = 'http://oss-cn-beijing.aliyuncs.com'\n",
    "    bucketName = 'csgroup'\n",
    "    return oss2.Bucket(auth, endpoint, bucketName)\n",
    "\n",
    "bucket = init_oss()\n",
    "def process_folder(folder_path):\n",
    "    result = []\n",
    "    supported_extensions = ['.txt', '.csv', '.docx', '.pdf', '.xlsx', '.cpp', '.py', '.c', '.h', '.hpp', '.java', '.js']\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # 获取文件路径\n",
    "            file_path = os.path.join(root, file)\n",
    "            # 获取文件扩展名，不区分大小写\n",
    "            file_extension = os.path.splitext(file)[1].lower()\n",
    "            relative_path = os.path.relpath(file_path, folder_path)\n",
    "\n",
    "            if file_extension in supported_extensions:\n",
    "                try:\n",
    "                    # content = extract_content(file_path, file_extension)\n",
    "                    # result.append(content)\n",
    "                    content = extract_content(file_path, file_extension)\n",
    "                    # 创建包含文件信息的字典\n",
    "                    doc_info = {\n",
    "                        'filename': file,\n",
    "                        'path': relative_path,\n",
    "                        'extension': file_extension,\n",
    "                        'content': content\n",
    "                    }\n",
    "                    result.append(doc_info)\n",
    "                    print(f\"成功处理文件: {relative_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "## 普通文件\n",
    "def extract_document(file_path, file_extension):\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            detected = chardet.detect(raw_data)\n",
    "            return raw_data.decode(detected['encoding'])\n",
    "\n",
    "    elif file_extension == '.csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df.to_string()\n",
    "\n",
    "    elif file_extension == '.docx':\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "\n",
    "    elif file_extension == '.pdf':\n",
    "        text = ''\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + '\\n'\n",
    "        return text\n",
    "\n",
    "    elif file_extension == '.xlsx':\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df.to_string()\n",
    "\n",
    "## 代码文件\n",
    "def extract_code(file_path, file_extension):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    # Java代码解析\n",
    "    if file_extension == '.java':\n",
    "        try:\n",
    "            tree = javalang.parse.parse(content)\n",
    "            # 提取类名、方法名、变量名等\n",
    "            analysis = []\n",
    "            for path, node in tree.filter(javalang.tree.ClassDeclaration):\n",
    "                analysis.append(f\"类名: {node.name}\")\n",
    "                for method in node.methods:\n",
    "                    analysis.append(f\"方法: {method.name}\")\n",
    "                    if method.documentation:\n",
    "                        analysis.append(f\"文档: {method.documentation}\")\n",
    "            return \"\\n\".join(analysis) + \"\\n原始代码:\\n\" + content\n",
    "        except:\n",
    "            return content\n",
    "\n",
    "    # Python代码解析\n",
    "    elif file_extension == '.py':\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            analysis = []\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.ClassDef):\n",
    "                    analysis.append(f\"类名: {node.name}\")\n",
    "                elif isinstance(node, ast.FunctionDef):\n",
    "                    analysis.append(f\"函数: {node.name}\")\n",
    "                    if ast.get_docstring(node):\n",
    "                        analysis.append(f\"文档: {ast.get_docstring(node)}\")\n",
    "            return \"\\n\".join(analysis) + \"\\n原始代码:\\n\" + content\n",
    "        except:\n",
    "            return content\n",
    "\n",
    "    # C/C++代码解析\n",
    "    elif file_extension in ['.cpp', '.c', '.h', '.hpp']:\n",
    "        try:\n",
    "            index = clang.cindex.Index.create()\n",
    "            tu = index.parse(file_path)\n",
    "            analysis = []\n",
    "\n",
    "            def process_node(node):\n",
    "                if node.kind == clang.cindex.CursorKind.FUNCTION_DECL:\n",
    "                    analysis.append(f\"函数: {node.spelling}\")\n",
    "                elif node.kind == clang.cindex.CursorKind.CLASS_DECL:\n",
    "                    analysis.append(f\"类名: {node.spelling}\")\n",
    "                for child in node.get_children():\n",
    "                    process_node(child)\n",
    "\n",
    "            process_node(tu.cursor)\n",
    "            return \"\\n\".join(analysis) + \"\\n原始代码:\\n\" + content\n",
    "        except:\n",
    "            return content\n",
    "\n",
    "    # JavaScript代码解析\n",
    "    elif file_extension == '.js':\n",
    "        try:\n",
    "            ast = esprima.parseScript(content)\n",
    "            analysis = []\n",
    "\n",
    "            def process_node(node):\n",
    "                if node.type == 'FunctionDeclaration':\n",
    "                    analysis.append(f\"函数: {node.id.name}\")\n",
    "                elif node.type == 'ClassDeclaration':\n",
    "                    analysis.append(f\"类名: {node.id.name}\")\n",
    "\n",
    "            for node in ast.body:\n",
    "                process_node(node)\n",
    "            return \"\\n\".join(analysis) + \"\\n原始代码:\\n\" + content\n",
    "        except:\n",
    "            return content\n",
    "\n",
    "def extract_content(file_path, file_extension):\n",
    "    if file_extension in ['.txt', '.csv', '.docx', '.pdf', '.xlsx']:\n",
    "        return extract_document(file_path, file_extension)\n",
    "\n",
    "    elif file_extension in ['.cpp', '.py', '.c', '.h', '.hpp', '.java', '.js']:\n",
    "        return extract_code(file_path, file_extension)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的文件类型: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建检索器\n",
    "import torch\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "import chardet\n",
    "\n",
    "import chromadb\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"你是一个AI助手。请基于用户的原始问题，生成几个不同的查询表达方式，以便从文档数据库中检索相关信息。\n",
    "    这些查询应该从不同角度来表达相同的意图。每个查询用换行符分隔。\n",
    "\n",
    "    原始问题: {question}\n",
    "    生成的查询:\"\"\",\n",
    ")\n",
    "\n",
    "def create_retriever(folder_path):\n",
    "    # 处理文件夹中的所有文件\n",
    "    file_contents = process_folder(folder_path)\n",
    "    \n",
    "    # 创建Document对象列表\n",
    "    documents = []\n",
    "    for doc in file_contents:\n",
    "        formatted_content = f\"\"\"\n",
    "        文件名: {doc['filename']}\n",
    "        文件路径: {doc['path']}\n",
    "        文件类型: {doc['extension']}\n",
    "        ---\n",
    "        {doc['content']}\n",
    "        \"\"\"\n",
    "        # 创建Document对象\n",
    "        documents.append(\n",
    "            Document(\n",
    "                # page_content=formatted_content,\n",
    "                page_content=doc['content'],\n",
    "                metadata={\n",
    "                    'source': doc['path'],\n",
    "                    'filename': doc['filename'],\n",
    "                    'extension': doc['extension']\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 使用文本分割器\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=10000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \".\", \"!\", \"?\", \" \"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # 直接分割Document对象\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    \n",
    "    persist_directory = \"chroma_db\"\n",
    "    if os.path.exists(persist_directory):\n",
    "        shutil.rmtree(persist_directory)\n",
    "        print(f\"已删除原有数据库: {persist_directory}\")\n",
    "    # 创建向量存储\n",
    "    docsearch = Chroma.from_documents(\n",
    "        documents=split_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,  # 添加持久化目录\n",
    "            client_settings=chromadb.config.Settings(\n",
    "                anonymized_telemetry=False,\n",
    "                is_persistent=True\n",
    "            )\n",
    "    )\n",
    "    # 6. 确保持久化\n",
    "    docsearch.persist()\n",
    "    def check_document_count(vectordb, required_k=3):\n",
    "        total_docs = len(vectordb.get())\n",
    "        if total_docs < required_k:\n",
    "            print(f\"警告: 数据库中只有 {total_docs} 个文档，少于请求的 {required_k} 个\")\n",
    "            print(\"建议: 添加更多文档或减少请求数量\")\n",
    "        return total_docs\n",
    "\n",
    "    # 使用前检查\n",
    "    total_docs = check_document_count(docsearch)\n",
    "    # 创建检索器\n",
    "    base_retriever = docsearch.as_retriever(\n",
    "        search_type=\"mmr\",  # 使用MMR搜索策略\n",
    "        search_kwargs={\n",
    "            \"k\": min(1, total_docs),         # 返回的文档数\n",
    "            \"fetch_k\": 3,   # 初始获取的文档数\n",
    "            \"lambda_mult\": 0.9  # MMR多样性参数\n",
    "        }\n",
    "    )\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        base_retriever, \n",
    "        llm,\n",
    "        prompt=QUERY_PROMPT,\n",
    "#         verbose=True  # 显示生成的查询\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功处理文件: 病例1.txt\n",
      "成功处理文件: 病例10.txt\n",
      "成功处理文件: 病例100.txt\n",
      "成功处理文件: 病例11.txt\n",
      "成功处理文件: 病例12.txt\n",
      "成功处理文件: 病例13.txt\n",
      "成功处理文件: 病例14.txt\n",
      "成功处理文件: 病例15.txt\n",
      "成功处理文件: 病例16txt.txt\n",
      "成功处理文件: 病例17txt.txt\n",
      "成功处理文件: 病例18txt.txt\n",
      "成功处理文件: 病例19txt.txt\n",
      "成功处理文件: 病例2.txt\n",
      "成功处理文件: 病例20txt.txt\n",
      "成功处理文件: 病例21txt.txt\n",
      "成功处理文件: 病例22txt.txt\n",
      "成功处理文件: 病例23txt.txt\n",
      "成功处理文件: 病例24txt.txt\n",
      "成功处理文件: 病例25txt.txt\n",
      "成功处理文件: 病例26txt.txt\n",
      "成功处理文件: 病例27txt.txt\n",
      "成功处理文件: 病例28txt.txt\n",
      "成功处理文件: 病例29txt.txt\n",
      "成功处理文件: 病例3.txt\n",
      "成功处理文件: 病例30txt.txt\n",
      "成功处理文件: 病例31txt.txt\n",
      "成功处理文件: 病例32txt.txt\n",
      "成功处理文件: 病例33txt.txt\n",
      "成功处理文件: 病例34txt.txt\n",
      "成功处理文件: 病例35txt.txt\n",
      "成功处理文件: 病例36.txt\n",
      "成功处理文件: 病例37.txt\n",
      "成功处理文件: 病例38.txt\n",
      "成功处理文件: 病例39.txt\n",
      "成功处理文件: 病例4.txt\n",
      "成功处理文件: 病例40.txt\n",
      "成功处理文件: 病例41.txt\n",
      "成功处理文件: 病例42.txt\n",
      "成功处理文件: 病例43.txt\n",
      "成功处理文件: 病例44.txt\n",
      "成功处理文件: 病例45.txt\n",
      "成功处理文件: 病例46.txt\n",
      "成功处理文件: 病例47.txt\n",
      "成功处理文件: 病例48.txt\n",
      "成功处理文件: 病例49.txt\n",
      "成功处理文件: 病例5.txt\n",
      "成功处理文件: 病例50.txt\n",
      "成功处理文件: 病例51.txt\n",
      "成功处理文件: 病例52.txt\n",
      "成功处理文件: 病例53.txt\n",
      "成功处理文件: 病例54.txt\n",
      "成功处理文件: 病例55.txt\n",
      "成功处理文件: 病例56.txt\n",
      "成功处理文件: 病例57.txt\n",
      "成功处理文件: 病例58.txt\n",
      "成功处理文件: 病例59.txt\n",
      "成功处理文件: 病例6.txt\n",
      "成功处理文件: 病例60.txt\n",
      "成功处理文件: 病例61.txt\n",
      "成功处理文件: 病例62.txt\n",
      "成功处理文件: 病例63.txt\n",
      "成功处理文件: 病例64.txt\n",
      "成功处理文件: 病例65.txt\n",
      "成功处理文件: 病例66.txt\n",
      "成功处理文件: 病例67.txt\n",
      "成功处理文件: 病例68.txt\n",
      "成功处理文件: 病例69.txt\n",
      "成功处理文件: 病例7.txt\n",
      "成功处理文件: 病例70.txt\n",
      "成功处理文件: 病例71.txt\n",
      "成功处理文件: 病例72.txt\n",
      "成功处理文件: 病例73.txt\n",
      "成功处理文件: 病例74.txt\n",
      "成功处理文件: 病例75.txt\n",
      "成功处理文件: 病例76.txt\n",
      "成功处理文件: 病例77.txt\n",
      "成功处理文件: 病例78.txt\n",
      "成功处理文件: 病例79.txt\n",
      "成功处理文件: 病例8.txt\n",
      "成功处理文件: 病例80.txt\n",
      "成功处理文件: 病例81.txt\n",
      "成功处理文件: 病例82.txt\n",
      "成功处理文件: 病例83.txt\n",
      "成功处理文件: 病例84.txt\n",
      "成功处理文件: 病例85.txt\n",
      "成功处理文件: 病例86.txt\n",
      "成功处理文件: 病例87.txt\n",
      "成功处理文件: 病例88.txt\n",
      "成功处理文件: 病例89.txt\n",
      "成功处理文件: 病例9.txt\n",
      "成功处理文件: 病例90.txt\n",
      "成功处理文件: 病例91.txt\n",
      "成功处理文件: 病例92.txt\n",
      "成功处理文件: 病例93.txt\n",
      "成功处理文件: 病例94.txt\n",
      "成功处理文件: 病例95.txt\n",
      "成功处理文件: 病例96.txt\n",
      "成功处理文件: 病例97.txt\n",
      "成功处理文件: 病例98.txt\n",
      "成功处理文件: 病例99.txt\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'chroma_db\\\\456e9311-1cbd-4ff9-a2f0-83bf1290c853\\\\data_level0.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 获取相关文档\u001b[39;00m\n\u001b[0;32m      2\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Desktop/NLP/病例样本\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m李建华的病历\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)\n",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m, in \u001b[0;36mcreate_retriever\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     60\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma_db\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(persist_directory):\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m已删除原有数据库: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersist_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 创建向量存储\u001b[39;00m\n",
      "File \u001b[1;32md:\\DownLoad\\software\\Python312\\Lib\\shutil.py:781\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# can't continue even if onexc hook returns\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monexc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\DownLoad\\software\\Python312\\Lib\\shutil.py:635\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    633\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 635\u001b[0m             \u001b[43monexc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    637\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32md:\\DownLoad\\software\\Python312\\Lib\\shutil.py:633\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    631\u001b[0m fullname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, name)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 633\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    635\u001b[0m     onexc(os\u001b[38;5;241m.\u001b[39munlink, fullname, err)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'chroma_db\\\\456e9311-1cbd-4ff9-a2f0-83bf1290c853\\\\data_level0.bin'"
     ]
    }
   ],
   "source": [
    "# 获取相关文档\n",
    "folder_path = \"D:/Desktop/NLP/病例样本\"\n",
    "retriever = create_retriever(folder_path)\n",
    "question = \"李建华的病历\"\n",
    "relevant_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# 打印检索结果\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"\\n文档 {i}:\")\n",
    "    print(f\"内容: {doc.page_content[:200]}...\")  # 显示前200个字符\n",
    "    print(f\"来源: {doc.metadata['source']}\")\n",
    "    print(f\"文件名: {doc.metadata['filename']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
